{"meta":{"title":"Technology is a means, not an end","subtitle":"","description":"","author":"wangji","url":"https://wangjiosw.github.io","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2022-05-08T10:24:10.739Z","updated":"2021-05-10T14:29:52.000Z","comments":false,"path":"/404.html","permalink":"https://wangjiosw.github.io/404.html","excerpt":"","text":""},{"title":"关于","date":"2022-05-08T10:24:10.733Z","updated":"2021-05-10T14:29:52.000Z","comments":false,"path":"about/index.html","permalink":"https://wangjiosw.github.io/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"书单","date":"2022-05-08T10:24:10.734Z","updated":"2021-05-10T14:29:52.000Z","comments":false,"path":"books/index.html","permalink":"https://wangjiosw.github.io/books/index.html","excerpt":"","text":""},{"title":"分类","date":"2022-05-08T10:24:10.740Z","updated":"2021-05-10T14:29:52.000Z","comments":false,"path":"categories/index.html","permalink":"https://wangjiosw.github.io/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2022-05-08T10:24:10.736Z","updated":"2021-05-10T14:29:52.000Z","comments":true,"path":"links/index.html","permalink":"https://wangjiosw.github.io/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2022-05-08T10:24:10.692Z","updated":"2021-05-10T14:29:52.000Z","comments":false,"path":"repository/index.html","permalink":"https://wangjiosw.github.io/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2022-05-08T10:24:10.735Z","updated":"2021-05-10T14:29:52.000Z","comments":false,"path":"tags/index.html","permalink":"https://wangjiosw.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Idea No usages found in Project and Libraries","slug":"idea/idea-usage-not-find","date":"2021-04-26T01:28:00.000Z","updated":"2021-05-10T14:29:52.000Z","comments":true,"path":"2021/04/26/idea/idea-usage-not-find/","link":"","permalink":"https://wangjiosw.github.io/2021/04/26/idea/idea-usage-not-find/","excerpt":"","text":"1. 问题描述find usage 时，方法存在引用，结果却出现”No usages found in Project and Librarie” 2. 解决方法Idea : File –&gt; Invalidate Caches –&gt; Invalidate and Restart","categories":[{"name":"开发工具","slug":"开发工具","permalink":"https://wangjiosw.github.io/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"java","slug":"java","permalink":"https://wangjiosw.github.io/tags/java/"},{"name":"idea","slug":"idea","permalink":"https://wangjiosw.github.io/tags/idea/"}],"author":"wangji"},{"title":"DBeaver执行SELECT语句报错\"com/google/common/primitives/Ints\"","slug":"bigdata/hive/dbeaver-select-error","date":"2021-03-15T07:14:00.000Z","updated":"2021-05-10T14:29:52.000Z","comments":true,"path":"2021/03/15/bigdata/hive/dbeaver-select-error/","link":"","permalink":"https://wangjiosw.github.io/2021/03/15/bigdata/hive/dbeaver-select-error/","excerpt":"","text":"1. 问题描述使用Dbeaver执行SELECT语句查询数据时报以下错误 123456789101112131415161718192021222324252627org.jkiss.dbeaver.model.exec.DBCException: com/google/common/primitives/Ints at org.jkiss.dbeaver.ui.editors.sql.execute.SQLQueryJob.extractData(SQLQueryJob.java:809) at org.jkiss.dbeaver.ui.editors.sql.SQLEditor$QueryResultsContainer.readData(SQLEditor.java:3008) at org.jkiss.dbeaver.ui.controls.resultset.ResultSetJobDataRead.lambda$0(ResultSetJobDataRead.java:120) at org.jkiss.dbeaver.model.exec.DBExecUtils.tryExecuteRecover(DBExecUtils.java:168) at org.jkiss.dbeaver.ui.controls.resultset.ResultSetJobDataRead.run(ResultSetJobDataRead.java:118) at org.jkiss.dbeaver.ui.controls.resultset.ResultSetViewer$ResultSetDataPumpJob.run(ResultSetViewer.java:4425) at org.jkiss.dbeaver.model.runtime.AbstractJob.run(AbstractJob.java:105) at org.eclipse.core.internal.jobs.Worker.run(Worker.java:63)Caused by: org.jkiss.dbeaver.DBException: com/google/common/primitives/Ints at org.jkiss.dbeaver.model.exec.DBExecUtils.tryExecuteRecover(DBExecUtils.java:227) at org.jkiss.dbeaver.ui.editors.sql.execute.SQLQueryJob.executeSingleQuery(SQLQueryJob.java:423) at org.jkiss.dbeaver.ui.editors.sql.execute.SQLQueryJob.extractData(SQLQueryJob.java:804) ... 7 moreCaused by: java.lang.NoClassDefFoundError: com/google/common/primitives/Ints at org.apache.hive.service.cli.Column.&lt;init&gt;(Column.java:149) at org.apache.hive.service.cli.ColumnBasedSet.&lt;init&gt;(ColumnBasedSet.java:52) at org.apache.hive.service.cli.RowSetFactory.create(RowSetFactory.java:37) at org.apache.hive.jdbc.HiveQueryResultSet.fetchNextResultset(HiveQueryResultSet.java:446) at org.apache.hive.jdbc.HiveQueryResultSet.next(HiveQueryResultSet.java:519) at org.jkiss.dbeaver.model.impl.jdbc.exec.JDBCResultSetImpl.next(JDBCResultSetImpl.java:272) at org.jkiss.dbeaver.model.impl.jdbc.exec.JDBCResultSetImpl.nextRow(JDBCResultSetImpl.java:180) at org.jkiss.dbeaver.ui.editors.sql.execute.SQLQueryJob.fetchQueryData(SQLQueryJob.java:708) at org.jkiss.dbeaver.ui.editors.sql.execute.SQLQueryJob.executeStatement(SQLQueryJob.java:536) at org.jkiss.dbeaver.ui.editors.sql.execute.SQLQueryJob.lambda$0(SQLQueryJob.java:436) at org.jkiss.dbeaver.model.exec.DBExecUtils.tryExecuteRecover(DBExecUtils.java:168) ... 9 more 2.解决方法在DBeaver的hive连接的驱动设置中添加服务器上HIVE_HOME下的guava-*.jar包","categories":[{"name":"开发工具","slug":"开发工具","permalink":"https://wangjiosw.github.io/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"dbeaver","slug":"dbeaver","permalink":"https://wangjiosw.github.io/tags/dbeaver/"},{"name":"hive","slug":"hive","permalink":"https://wangjiosw.github.io/tags/hive/"}]},{"title":"大数据Docker环境","slug":"bigdata/bigdata-env","date":"2021-03-14T00:12:00.000Z","updated":"2021-05-10T14:29:52.000Z","comments":true,"path":"2021/03/14/bigdata/bigdata-env/","link":"","permalink":"https://wangjiosw.github.io/2021/03/14/bigdata/bigdata-env/","excerpt":"","text":"Spark：singularities/spark Kafka ：wurstmeister/kafka","categories":[{"name":"Docker","slug":"Docker","permalink":"https://wangjiosw.github.io/categories/Docker/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://wangjiosw.github.io/tags/spark/"},{"name":"kafka","slug":"kafka","permalink":"https://wangjiosw.github.io/tags/kafka/"}]},{"title":"Java设计模式：原型模式","slug":"design-patterns/ProtoType","date":"2020-11-25T06:06:00.000Z","updated":"2021-05-10T14:29:52.000Z","comments":true,"path":"2020/11/25/design-patterns/ProtoType/","link":"","permalink":"https://wangjiosw.github.io/2020/11/25/design-patterns/ProtoType/","excerpt":"","text":"从原型实例复制克隆出新实例，而绝不是从类去实例化 使用场景 当希望基于现有对象创建新的对象时，比如某个类的实例很复杂，如果完全重新创建成本会很高，可以将这个实例复制一份。 参照原型进行量产（批量创建同类型对象实例）。 优点 向客户隐藏制造新实例的复杂性 JVM会进行内存操作直接拷贝原始数据流，简单粗暴，不会有其他更多的复杂操作（类加载，实例化，初始化等等），速度远远快于实例化操作 缺点 对象的复制有时相当复杂 例子原型对象Phone.java 12345678910111213141516171819202122232425262728public class Phone implements Cloneable&#123; // 手机名 private String name; // 浅拷贝只拷贝地址，需要深拷贝 private UI phoneUI = new UI(); public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public void setPhoneUI(UI phoneUI) &#123; this.phoneUI = phoneUI; &#125; @Override protected Phone clone() throws CloneNotSupportedException &#123; // 浅拷贝 Phone clonePhone = (Phone)super.clone(); // 深拷贝 clonePhone.setPhoneUI(this.phoneUI.clone()); return clonePhone; &#125;&#125; 对Phone进行量产的工厂 1234567891011121314public class PhoneFactory &#123; // 原型 private static Phone protoType = new Phone(); // 获取克隆实例 public static Phone getInstance(int x) throws CloneNotSupportedException &#123; // 复制原型 Phone clonePhone = protoType.clone(); clonePhone.setName(&quot;phone:&quot;+x); return clonePhone; &#125;&#125; 测试原型模式创建实例 12345678public class TestProtoType &#123; public static void main(String[] args) throws CloneNotSupportedException &#123; for (int i = 0; i &lt; 10; i++) &#123; Phone phone = PhoneFactory.getInstance(i); System.out.println(phone.toString()+&quot;\\t&quot;+phone.getName()); &#125; &#125;&#125; 结果 123456789101112com.wangji.prototype.Phone@61bbe9ba phone:0com.wangji.prototype.Phone@610455d6 phone:1com.wangji.prototype.Phone@511d50c0 phone:2com.wangji.prototype.Phone@60e53b93 phone:3com.wangji.prototype.Phone@5e2de80c phone:4com.wangji.prototype.Phone@1d44bcfa phone:5com.wangji.prototype.Phone@266474c2 phone:6com.wangji.prototype.Phone@6f94fa3e phone:7com.wangji.prototype.Phone@5e481248 phone:8com.wangji.prototype.Phone@66d3c617 phone:9Process finished with exit code 0","categories":[{"name":"Java设计模式","slug":"Java设计模式","permalink":"https://wangjiosw.github.io/categories/Java%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"java","slug":"java","permalink":"https://wangjiosw.github.io/tags/java/"},{"name":"protoType","slug":"protoType","permalink":"https://wangjiosw.github.io/tags/protoType/"}],"author":"wangji"},{"title":"Passing Functions to Spark","slug":"bigdata/spark/Passing-Functions-to-Spark","date":"2020-11-10T02:19:00.000Z","updated":"2021-05-10T14:29:52.000Z","comments":true,"path":"2020/11/10/bigdata/spark/Passing-Functions-to-Spark/","link":"","permalink":"https://wangjiosw.github.io/2020/11/10/bigdata/spark/Passing-Functions-to-Spark/","excerpt":"","text":"https://spark.apache.org/docs/2.2.1/rdd-programming-guide.html Spark’s API relies heavily on passing functions in the driver program to run on the cluster. There are two recommended ways to do this: Anonymous function syntax, which can be used for short pieces of code. Static methods in a global singleton object. For example, you can define object MyFunctions and then pass MyFunctions.func1, as follows: 12345object MyFunctions &#123; def func1(s: String): String = &#123; ... &#125;&#125;myRdd.map(MyFunctions.func1) Note that while it is also possible to pass a reference to a method in a class instance (as opposed to a singleton object), this requires sending the object that contains that class along with the method. For example, consider: 1234class MyClass &#123; def func1(s: String): String = &#123; ... &#125; def doStuff(rdd: RDD[String]): RDD[String] = &#123; rdd.map(func1) &#125;&#125; Here, if we create a new MyClass instance and call doStuff on it, the map inside there references the func1 method of that MyClass instance, so the whole object needs to be sent to the cluster. It is similar to writing rdd.map(x =&gt; this.func1(x)). In a similar way, accessing fields of the outer object will reference the whole object: 1234class MyClass &#123; val field = &quot;Hello&quot; def doStuff(rdd: RDD[String]): RDD[String] = &#123; rdd.map(x =&gt; field + x) &#125;&#125; is equivalent to writing rdd.map(x =&gt; this.field + x), which references all of this. To avoid this issue, the simplest way is to copy field into a local variable instead of accessing it externally: 1234def doStuff(rdd: RDD[String]): RDD[String] = &#123; val field_ = this.field rdd.map(x =&gt; field_ + x)&#125;","categories":[{"name":"Spark","slug":"Spark","permalink":"https://wangjiosw.github.io/categories/Spark/"}],"tags":[{"name":"function","slug":"function","permalink":"https://wangjiosw.github.io/tags/function/"}]},{"title":"查看kafka分区偏移量文件报错：NoSuchElementException","slug":"bigdata/kafka/kafka-dump-log-NoSuchElementException","date":"2020-11-01T02:40:00.000Z","updated":"2021-05-10T14:29:52.000Z","comments":true,"path":"2020/11/01/bigdata/kafka/kafka-dump-log-NoSuchElementException/","link":"","permalink":"https://wangjiosw.github.io/2020/11/01/bigdata/kafka/kafka-dump-log-NoSuchElementException/","excerpt":"","text":"错误描述使用kafka-dump-log.sh 分析kafka 分区日志的.index文件时，报错 123456789bin/kafka-dump-log.sh --files /kafka/kafka-logs-5dbba1c65ae8/topic-demo-2/00000000000000000000.index Dumping /kafka/kafka-logs-5dbba1c65ae8/topic-demo-2/00000000000000000000.indexException in thread &quot;main&quot; java.util.NoSuchElementException at org.apache.kafka.common.utils.AbstractIterator.next(AbstractIterator.java:52) at kafka.tools.DumpLogSegments$.$anonfun$dumpIndex$1(DumpLogSegments.scala:140) at kafka.tools.DumpLogSegments$.dumpIndex(DumpLogSegments.scala:132) at kafka.tools.DumpLogSegments$.$anonfun$main$1(DumpLogSegments.scala:59) at kafka.tools.DumpLogSegments$.main(DumpLogSegments.scala:48) at kafka.tools.DumpLogSegments.main(DumpLogSegments.scala) 错误原因日志里面没消息 解决方法换一个日志里面有消息的日志对应的.index文件查看或往该分区写入消息","categories":[{"name":"Kafka","slug":"Kafka","permalink":"https://wangjiosw.github.io/categories/Kafka/"}],"tags":[{"name":"parittion","slug":"parittion","permalink":"https://wangjiosw.github.io/tags/parittion/"}],"author":"wangji"},{"title":"kafka主题的分区分配策略（二）","slug":"bigdata/kafka/kafka-topic-partition2","date":"2020-10-26T14:04:00.000Z","updated":"2021-05-10T14:29:52.000Z","comments":true,"path":"2020/10/26/bigdata/kafka/kafka-topic-partition2/","link":"","permalink":"https://wangjiosw.github.io/2020/10/26/bigdata/kafka/kafka-topic-partition2/","excerpt":"","text":"使用该分区策略的条件broker包含机架信息，且未使用replica-assignment参数 和kafka主题的分区分配策略（一）的相同和不同之处假设目前有3个机架，9个broker，机架和broker结点的对照关系入下表机架 | broker—|—rack1 | 0，1，2rack2 | 3，4，5rack3 | 6，7，8 相同点分区的broker分配步骤和kafka主题的分区分配策略（一）基本相同。 不同点不同点1kafka主题的分区分配策略（一）的assignReplicasToBrokersRackUnaware()方法里的brokerArray变量的值为[0,1,2,3,4,5,6,7,8] 而assignReplicasToBrokersRackUnaware()方法里的brokerArray变量的值为[0,3,6,1,4,7,2,5,8]，这是论询各个机架产生的结果，如此新的brokerArray中包含了简单的机架分配信息。 不同点2给分区分配的broker需要经过一层过滤","categories":[{"name":"Kafka","slug":"Kafka","permalink":"https://wangjiosw.github.io/categories/Kafka/"}],"tags":[{"name":"topic","slug":"topic","permalink":"https://wangjiosw.github.io/tags/topic/"},{"name":"partition","slug":"partition","permalink":"https://wangjiosw.github.io/tags/partition/"}],"author":"wangji"},{"title":"kafka主题的分区分配策略（一）","slug":"bigdata/kafka/kafka-topic-partition1","date":"2020-10-26T13:40:00.000Z","updated":"2021-05-10T14:29:52.000Z","comments":true,"path":"2020/10/26/bigdata/kafka/kafka-topic-partition1/","link":"","permalink":"https://wangjiosw.github.io/2020/10/26/bigdata/kafka/kafka-topic-partition1/","excerpt":"","text":"使用该分区策略的条件未指定机架或使用disable-rack-aware参数来创建主题，且未使用replica-assignment参数 思路核心是遍历每个分区partition，然后从brokerArray（brokerId的列表）中选取replicationFactor个brokerId分配给这个partition。 kafka.admin.AdminUtilities12345678910111213141516171819202122232425262728private def assignReplicasToBrokersRackUnaware(nPartitions: Int, replicationFactor: Int, brokerList: Seq[Int], fixedStartIndex: Int, startPartitionId: Int): Map[Int, Seq[Int]] = &#123; val ret = mutable.Map[Int, Seq[Int]]() val brokerArray = brokerList.toArray val startIndex = if (fixedStartIndex &gt;= 0) fixedStartIndex else rand.nextInt(brokerArray.length) var currentPartitionId = math.max(0, startPartitionId) var nextReplicaShift = if (fixedStartIndex &gt;= 0) fixedStartIndex else rand.nextInt(brokerArray.length) for (_ &lt;- 0 until nPartitions) &#123; if (currentPartitionId &gt; 0 &amp;&amp; (currentPartitionId % brokerArray.length == 0)) nextReplicaShift += 1 val firstReplicaIndex = (currentPartitionId + startIndex) % brokerArray.length val replicaBuffer = mutable.ArrayBuffer(brokerArray(firstReplicaIndex)) for (j &lt;- 0 until replicationFactor - 1) replicaBuffer += brokerArray(replicaIndex(firstReplicaIndex, nextReplicaShift, j, brokerArray.length)) ret.put(currentPartitionId, replicaBuffer) currentPartitionId += 1 &#125; ret &#125; private def replicaIndex(firstReplicaIndex: Int, secondReplicaShift: Int, replicaIndex: Int, nBrokers: Int): Int = &#123; val shift = 1 + (secondReplicaShift + replicaIndex) % (nBrokers - 1) (firstReplicaIndex + shift) % nBrokers&#125;","categories":[{"name":"Kafka","slug":"Kafka","permalink":"https://wangjiosw.github.io/categories/Kafka/"}],"tags":[{"name":"topic","slug":"topic","permalink":"https://wangjiosw.github.io/tags/topic/"},{"name":"partition","slug":"partition","permalink":"https://wangjiosw.github.io/tags/partition/"}],"author":"wangji"},{"title":"kafka主题与分区管理","slug":"bigdata/kafka/topic-partition","date":"2020-10-26T13:36:00.000Z","updated":"2021-05-10T14:29:52.000Z","comments":true,"path":"2020/10/26/bigdata/kafka/topic-partition/","link":"","permalink":"https://wangjiosw.github.io/2020/10/26/bigdata/kafka/topic-partition/","excerpt":"","text":"主题创建主题1bin/kafka-topics.sh --zookeeper localhost:2181/kafka --create --topic topic-demo --partitions 4 --replication-factor 1 查看单个主题信息1bin/kafka-topics.sh --zookeeper localhost:2181/kafka --describe --topic topic-demo 查看当前所有可用主题1bin/kafka-topics.sh --zookeeper localhost:2181/kafka -list 修改主题将topic-demo分区数修改为3 1bin/kafka-topics.sh --zookeeper localhost:2181/kafka --alter --topic topic-demo --partitions 3 删除主题1bin/kafka-topics.sh --zookeeper localhost:2181/kafka --delete --topic topic-create-api","categories":[{"name":"Kafka","slug":"Kafka","permalink":"https://wangjiosw.github.io/categories/Kafka/"}],"tags":[{"name":"topic","slug":"topic","permalink":"https://wangjiosw.github.io/tags/topic/"},{"name":"partition","slug":"partition","permalink":"https://wangjiosw.github.io/tags/partition/"}],"author":"wangji"},{"title":"Hive分析搜狗用户搜索日志-数据预处理(解决乱码问题)","slug":"bigdata/hive/hive-Sougou","date":"2020-10-22T11:34:00.000Z","updated":"2021-05-10T14:29:52.000Z","comments":true,"path":"2020/10/22/bigdata/hive/hive-Sougou/","link":"","permalink":"https://wangjiosw.github.io/2020/10/22/bigdata/hive/hive-Sougou/","excerpt":"","text":"环境 MacOS 单机hadoop集群 Hive (内嵌模式) 预处理1. 查看数据1head Sougou.reduced 结果如下 可以发现有许多乱码 2. 查看文件编码1file Sougou.reduced 结果如下 网上查询得知Non-ISO extended-ASCII text代表gb18030 3. 使用iconv转换文件编码格式1iconv -f gb18030 -t utf-8 SogouQ.reduced&gt;SogouQ.reduced.new 结果如下","categories":[{"name":"Hive","slug":"Hive","permalink":"https://wangjiosw.github.io/categories/Hive/"}],"tags":[{"name":"macos","slug":"macos","permalink":"https://wangjiosw.github.io/tags/macos/"},{"name":"sougou","slug":"sougou","permalink":"https://wangjiosw.github.io/tags/sougou/"}]},{"title":"Mac 单机集群使用hadoop jar 运行jar包出现的问题","slug":"bigdata/mapreduce/mac-hadoop-jar-problem","date":"2020-10-21T03:19:00.000Z","updated":"2021-05-10T14:29:52.000Z","comments":true,"path":"2020/10/21/bigdata/mapreduce/mac-hadoop-jar-problem/","link":"","permalink":"https://wangjiosw.github.io/2020/10/21/bigdata/mapreduce/mac-hadoop-jar-problem/","excerpt":"","text":"1. 问题描述Mac 单机集群使用hadoop jar 运行jar包报错： 123456Exception in thread &quot;main&quot; java.io.IOException: Mkdirs failed to create /var/folders/zz/zyxvpxvq6csfxvn_n0000000000000/T/hadoop-unjar7560830407203462827/META-INF/licenseat org.apache.hadoop.util.RunJar.ensureDirectory(RunJar.java:128)at org.apache.hadoop.util.RunJar.unJar(RunJar.java:104)at org.apache.hadoop.util.RunJar.unJar(RunJar.java:81)at org.apache.hadoop.util.RunJar.run(RunJar.java:209)at org.apache.hadoop.util.RunJar.main(RunJar.java:136) 2. 解决方法Only thrown in MacOS, and solution is:@see: http://stackoverflow.com/questions/10522835/hadoop-java-io-ioexception-mkdirs-failed-to-create-some-path modify jar12$ zip -d mipr-core-0.1-jar-with-dependencies.jar META-INF/LICENSE$ zip -d mipr-core-0.1-jar-with-dependencies.jar LICENSE with grep all1$ jar -tvf mipr-core-0.1-jar-with-dependencies.jar |grep META-INF/LICENSE","categories":[{"name":"Mapreduce","slug":"Mapreduce","permalink":"https://wangjiosw.github.io/categories/Mapreduce/"}],"tags":[{"name":"macos","slug":"macos","permalink":"https://wangjiosw.github.io/tags/macos/"},{"name":"hadoop jar","slug":"hadoop-jar","permalink":"https://wangjiosw.github.io/tags/hadoop-jar/"}],"author":"wangji"},{"title":"使用fastjson时堆溢出","slug":"jvm/fastjson-heap-oom","date":"2020-10-21T02:52:00.000Z","updated":"2021-05-10T14:29:52.000Z","comments":true,"path":"2020/10/21/jvm/fastjson-heap-oom/","link":"","permalink":"https://wangjiosw.github.io/2020/10/21/jvm/fastjson-heap-oom/","excerpt":"","text":"1. 问题描述123456789101112131415161718192021222324252627282930private JSONArray getChildInfo(String root, Set&lt;String&gt; childrens) throws JSONException &#123; JSONArray rtJA = new JSONArray(); for (String child : childrens) &#123; String key = generateKey(root,child); if (isVisited.contains(key)) &#123; continue; &#125; String simpleTableName = child.substring(child.indexOf(&#x27;@&#x27;)+1); effectNode.add(simpleTableName); isVisited.add(key); JSONObject jsonObject = new JSONObject(); if(nodechildrens.containsKey(child))&#123; JSONArray temp = getChildInfo(child, nodechildrens.get(child)); if (!temp.isEmpty()) &#123; jsonObject.put(&quot;children&quot;, temp.toJSONString()); &#125; else &#123; jsonObject.put(&quot;children&quot;, &quot;&quot;); &#125; &#125; else &#123; jsonObject.put(&quot;children&quot;, &quot;&quot;); &#125; jsonObject.put(&quot;name&quot;,simpleTableName); isVisited.remove(key); rtJA.add(jsonObject.toString()); &#125; return rtJA;&#125; temp.toJSONString()报错 1fastjson java.lang.OutOfMemoryError: Java heap space 因为代码是里面用了递归，一开始以为是代码问题，debug了许久，没发现哪里逻辑错了。想着之前学过JVM，就试着通过堆快照分析哪里错了 2. 堆快照分析首先，设置jvm参数(jdk 1.8)，使发生堆溢出时保存快照 1-XX:HeapDumpPath=/Users/wangji/Desktop/dumpfile.hprof -XX:+HeapDumpOnOutOfMemoryError 之后，通过MAT分析快照 发现有个String有455MB的大小，使用fastjson时，又占了445MB，整体接近1G，这里才是发生堆溢出的原因。知道原因后，递归过程中对字符串进行简化操作（去除”\\“和其他多余的字符），最终解决问题 简化代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142private JSONArray getChildInfo(String root, Set&lt;String&gt; childrens) throws JSONException &#123; JSONArray rtJA = new JSONArray(); for (String child : childrens) &#123; String key = generateKey(root,child); if (isVisited.contains(key)) &#123; continue; &#125; String simpleTableName = child.substring(child.indexOf(&#x27;@&#x27;)+1); effectNode.add(simpleTableName); isVisited.add(key); JSONObject jsonObject = new JSONObject(); if(nodechildrens.containsKey(child))&#123; JSONArray temp = getChildInfo(child, nodechildrens.get(child)); if (!temp.isEmpty()) &#123; // 主要是这句的字符串引起了堆内存溢出，对字符串简化 jsonObject.put(&quot;children&quot;, modifyString(temp.toJSONString(),false)); &#125; else &#123; jsonObject.put(&quot;children&quot;, &quot;&quot;); &#125; &#125; else &#123; jsonObject.put(&quot;children&quot;, &quot;&quot;); &#125; jsonObject.put(&quot;name&quot;,simpleTableName); isVisited.remove(key); rtJA.add(jsonObject.toString()); &#125; return rtJA;&#125;private String modifyString(String origin, boolean removeSide) &#123; String out = origin.replaceAll(&quot;\\\\\\\\&quot;,&quot;&quot;); out = out.replaceAll(&quot;\\&quot;\\\\&#123;&quot;,&quot;&#123;&quot;); out = out.replaceAll(&quot;\\\\&#125;\\&quot;&quot;,&quot;&#125;&quot;); out = out.replaceAll(&quot;\\&quot;\\\\[&quot;,&quot;[&quot;); out = out.replaceAll(&quot;\\\\]\\&quot;&quot;,&quot;]&quot;); if (removeSide) &#123; out = out.substring(1,out.length()-1); &#125; return out;&#125;","categories":[{"name":"JVM","slug":"JVM","permalink":"https://wangjiosw.github.io/categories/JVM/"}],"tags":[{"name":"java","slug":"java","permalink":"https://wangjiosw.github.io/tags/java/"},{"name":"OutOfMemoryError","slug":"OutOfMemoryError","permalink":"https://wangjiosw.github.io/tags/OutOfMemoryError/"},{"name":"heap","slug":"heap","permalink":"https://wangjiosw.github.io/tags/heap/"}],"author":"wangji"},{"title":"循环神经网络pytorch实现","slug":"deep-learning/rnn-pytorch","date":"2020-04-20T05:17:00.000Z","updated":"2021-05-10T14:29:52.000Z","comments":true,"path":"2020/04/20/deep-learning/rnn-pytorch/","link":"","permalink":"https://wangjiosw.github.io/2020/04/20/deep-learning/rnn-pytorch/","excerpt":"","text":"简介使用pytorch简单使用循环神经网络（RNN、GRU、LSTM） RNN前向过程： $h_t = g(Uh_{t-1} + Wx_t +b_h)$ $y_t = g(W_yh_t + b_y)$ pytorch 实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import torchimport torch.nn as nnimport torch.nn.functional as Fclass RNNCell(nn.Module): def __init__(self, input_size, hidden_dim): super(RNNCell, self).__init__() self.input_size = input_size self.hidden_dim = hidden_dim self.linear1 = nn.Linear(hidden_dim, hidden_dim) self.linear2 = nn.Linear(input_size, hidden_dim) def forward(self, x, h_pre): &quot;&quot;&quot; :param x: (batch, input_size) :param h_pre: (batch, hidden_dim) :return: h_next (batch, hidden_dim) &quot;&quot;&quot; h_next = torch.tanh(self.linear1(h_pre) + self.linear2(x)) return h_nextclass RNN(nn.Module): def __init__(self, input_size, hidden_dim): super(RNN, self).__init__() self.input_size = input_size self.hidden_dim = hidden_dim self.rnn_cell = RNNCell(input_size, hidden_dim) def forward(self, x): &quot;&quot;&quot; :param x: (seq_len, batch,input_size) :return: output (seq_len, batch, hidden_dim) h_n (1, batch, hidden_dim) &quot;&quot;&quot; seq_len, batch, _ = x.shape h = torch.zeros(batch, self.hidden_dim) output = torch.zeros(seq_len, batch, self.hidden_dim) for i in range(seq_len): inp = x[i, :, :] h = self.rnn_cell(inp, h) output[i, :, :] = h h_n = output[-1:, :, :] return output, h_n LSTM前向过程： 输入门: $i_t = \\sigma (W_ix_t + U_ih_{t-1} + b_i)$ 遗忘门: $f_t = \\sigma (W_fx_t + U_fh_{t-1} + b_f)$ 输出门: $o_t = \\sigma (W_ox_t + U_oh_{t-1} + b_o)$ $\\hat{c}t = tanh(W_cx_t + U_ch{t-1} + b_c)$ $c_t = f_t \\odot c_{t-1} + i_t \\odot \\hat{c} _t$ $h_t = o_t \\odot tanh(c_t)$ pytorch 实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import torchimport torch.nn as nnimport torch.nn.functional as Fimport copyclass Gate(nn.Module): def __init__(self, input_size, hidden_dim): super(Gate, self).__init__() self.linear1 = nn.Linear(hidden_dim, hidden_dim) self.linear2 = nn.Linear(input_size, hidden_dim) def forward(self, x, h_pre, active_func): h_next = active_func(self.linear1(h_pre) + self.linear2(x)) return h_nextdef clones(module, N): &quot;Produce N identical layers.&quot; return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])class LSTMCell(nn.Module): def __init__(self, input_size, hidden_dim): super(LSTMCell, self).__init__() self.input_size = input_size self.hidden_dim = hidden_dim self.gate = clones(Gate(input_size, hidden_dim), 4) def forward(self, x, h_pre, c_pre): &quot;&quot;&quot; :param x: (batch, input_size) :param h_pre: (batch, hidden_dim) :param c_pre: (batch, hidden_dim) :return: h_next(batch, hidden_dim), c_next(batch, hidden_dim) &quot;&quot;&quot; f_t = self.gate[0](x, h_pre, torch.sigmoid) i_t = self.gate[1](x, h_pre, torch.sigmoid) g_t = self.gate[2](x, h_pre, torch.tanh) o_t = self.gate[3](x, h_pre, torch.sigmoid) c_next = f_t * c_pre + i_t * g_t h_next = o_t * torch.tanh(c_next) return h_next, c_nextclass LSTM(nn.Module): def __init__(self, input_size, hidden_dim): super(LSTM, self).__init__() self.input_size = input_size self.hidden_dim = hidden_dim self.lstm_cell = LSTMCell(input_size, hidden_dim) def forward(self, x): &quot;&quot;&quot; :param x: (seq_len, batch,input_size) :return: output (seq_len, batch, hidden_dim) h_n (1, batch, hidden_dim) c_n (1, batch, hidden_dim) &quot;&quot;&quot; seq_len, batch, _ = x.shape h = torch.zeros(batch, self.hidden_dim) c = torch.zeros(batch, self.hidden_dim) output = torch.zeros(seq_len, batch, self.hidden_dim) for i in range(seq_len): inp = x[i, :, :] h, c = self.lstm_cell(inp, h, c) output[i, :, :] = h h_n = output[-1:, :, :] return output, (h_n, c.unsqueeze(0)) GRU前向过程： 更新门: $r_t = \\sigma (W_{xr}x_t + W_{hr}h_{t-1} + b_r)$ $z_t = \\sigma (W_{xz}x_t + W_{hz}h_{t-1} + b_z)$ 候选隐含状态： $\\hat{h}t = tanh(W{xh}x_t + r_t \\odot W_{hh}h_{t-1} + b_h)$ 隐含状态： $h_t = z_t \\odot h_{t-1} + (1-z_t) \\odot \\hat{h}_t$ 输出: $y_t = softmax(W_{hy}h_t + b_y)$","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://wangjiosw.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"https://wangjiosw.github.io/tags/pytorch/"},{"name":"RNN","slug":"RNN","permalink":"https://wangjiosw.github.io/tags/RNN/"},{"name":"GRU","slug":"GRU","permalink":"https://wangjiosw.github.io/tags/GRU/"},{"name":"LSTM","slug":"LSTM","permalink":"https://wangjiosw.github.io/tags/LSTM/"}]},{"title":"矩阵求导方法","slug":"deep-learning/matrix-partial","date":"2020-04-18T05:17:00.000Z","updated":"2022-05-08T01:16:40.845Z","comments":true,"path":"2020/04/18/deep-learning/matrix-partial/","link":"","permalink":"https://wangjiosw.github.io/2020/04/18/deep-learning/matrix-partial/","excerpt":"","text":"1、全微分 当X是矩阵，$dy = tr(\\frac{\\partial y}{\\partial X}^T dX)$ 当X是向量，$dy = \\frac{\\partial y}{\\partial X}^T dX=tr(\\frac{\\partial y}{\\partial X}^T dX)$ 2、活用迹（tr）$(1) a是标量，a = tr(a)$ $(2) A，B为方阵，tr(AB) = tr(BA)$ $(3) tr(A) = tr(A^T)$ $(4) tr(A+B) = tr(A)+tr(B)$ $(5) 微分d(X^T) = (dX)^T$ 这些公式将用于下面的求导 3、矩阵求导例子下面将展示一些用上面公式求矩阵导数的例子： 例1：$X = (x_1,…,x_n)^T$是向量，$A$是与$X$无关的矩阵：$y = X^TAX ，求 \\frac{\\partial y}{\\partial X}?$ 全微分表达式：$dy = (dX^T)AX + X^TA(dX)$ 由公式（1）得： $dy = tr((dX^T)AX + X^TA(dX))$ 由公式（2）得： $dy = tr((dX^T)AX)+tr( X^TA(dX))$ 由公式(5) (3)得： $dy = tr((dX)^TAX)+tr( X^TA(dX))= tr(X^TA^TdX)+tr( X^TA(dX))$ 由公式（2）得： $dy = tr(X^TA^T(dX) + X^TA(dX))=tr(X^T(A^T+A)dX)$ $\\because dy = tr(\\frac{\\partial y}{\\partial X}^T dX)$ $\\Rightarrow \\frac{\\partial y}{\\partial X}^T = X^T(A^T+A)$ $\\Rightarrow \\frac{\\partial y}{\\partial X} = (X^T(A^T+A))^T= (A+A^T)X$ 例2：$y = tr(AB),求\\frac{\\partial y}{\\partial A}?$ 全微分表达式：$dy = tr[(dA)B]$ 由公式(2)得：$dy = tr[B\\ dA]$ $\\because dy = tr(\\frac{\\partial y}{\\partial X}^T dX)$ $\\Rightarrow \\frac{\\partial y}{\\partial A}^T = B$ $\\Rightarrow \\frac{\\partial y}{\\partial A} =B^T$","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://wangjiosw.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"矩阵求导","slug":"矩阵求导","permalink":"https://wangjiosw.github.io/tags/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/"}]},{"title":"Hbase创建表出错","slug":"bigdata/hbase/hbase-create-table-error","date":"2020-03-30T09:14:00.000Z","updated":"2021-05-10T14:29:52.000Z","comments":true,"path":"2020/03/30/bigdata/hbase/hbase-create-table-error/","link":"","permalink":"https://wangjiosw.github.io/2020/03/30/bigdata/hbase/hbase-create-table-error/","excerpt":"","text":"1. 问题描述123456hbase(main):002:0&gt; create &#x27;test&#x27;,&#x27;cf&#x27;ERROR: java.io.IOException: Table Namespace Manager not ready yet, try again later at org.apache.hadoop.hbase.master.HMaster.getNamespaceDescriptor(HMaster.java:3172) at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:1727) at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:1766) 2. 错误的原因Hbase底层hadoop集群时间不一致 3. 解决方法3.1 安装ntpdate工具1yum -y install ntp ntpdate 3.2 设置系统时间与网络时间同步1ntpdate cn.pool.ntp.org 4. 常用NTP服务器地址4.1 阿里云授时服务器123456789101112131415161718#NTP服务器ntp.aliyun.com ntp1.aliyun.comntp2.aliyun.comntp3.aliyun.comntp4.aliyun.comntp5.aliyun.comntp6.aliyun.comntp7.aliyun.com#Time服务器time1.aliyun.comtime2.aliyun.comtime3.aliyun.comtime4.aliyun.comtime5.aliyun.comtime6.aliyun.comtime7.aliyun.com 4.2 国内大学授时服务器123456s1c.time.edu.cn 北京大学 s2m.time.edu.cn 北京大学s1b.time.edu.cn 清华大学s1e.time.edu.cn 清华大学s2a.time.edu.cn 清华大学s2b.time.edu.cn 清华大学 4.3 国外授时服务器1234567891011121314#苹果提供的授时服务器 time1.apple.comtime2.apple.comtime3.apple.comtime4.apple.comtime5.apple.comtime6.apple.comtime7.apple.com#Google提供的授时服务器 time1.google.comtime2.google.comtime3.google.comtime4.google.com","categories":[{"name":"Hbase","slug":"Hbase","permalink":"https://wangjiosw.github.io/categories/Hbase/"}],"tags":[{"name":"ntpdate","slug":"ntpdate","permalink":"https://wangjiosw.github.io/tags/ntpdate/"}],"author":"wangji"},{"title":"安装hive时jline版本不一致问题","slug":"bigdata/hive/hive-install-jline-error","date":"2020-03-17T07:14:00.000Z","updated":"2021-05-10T14:29:52.000Z","comments":true,"path":"2020/03/17/bigdata/hive/hive-install-jline-error/","link":"","permalink":"https://wangjiosw.github.io/2020/03/17/bigdata/hive/hive-install-jline-error/","excerpt":"","text":"1. 问题描述1234将hive包解压后，运行bin目录下的hive脚本，报以下错误：[ERROR] Terminal initialization failed; falling back to unsupportedjava.lang.IncompatibleClassChangeError: Found class jline.Terminal, but interface was expected at jline.TerminalFactory.create(TerminalFactory.java:101) 2. 错误的原因Hadoop jline版本和hive的jline不一致 3. 解决方法删除your_install_path/hadoop/share/hadoop/yarn/lib目录下的jline包,然后把hive里面的jline包拷过来。","categories":[{"name":"Hive","slug":"Hive","permalink":"https://wangjiosw.github.io/categories/Hive/"}],"tags":[{"name":"jline","slug":"jline","permalink":"https://wangjiosw.github.io/tags/jline/"}]},{"title":"本地运行mapred问题","slug":"bigdata/mapreduce/mapred-localmode-error","date":"2020-03-17T06:58:00.000Z","updated":"2021-05-10T14:29:52.000Z","comments":true,"path":"2020/03/17/bigdata/mapreduce/mapred-localmode-error/","link":"","permalink":"https://wangjiosw.github.io/2020/03/17/bigdata/mapreduce/mapred-localmode-error/","excerpt":"","text":"1. 问题描述在hadoop集群上跑mapreduce代码很慢，可以通过设置 1conf.set(&quot;mapreduce.framework.name&quot;, &quot;local&quot;); 使mapreduce代码能在本地运行。 运行时报以下错误： 1234562020-03-14 20:24:30,241 WARN [main] util.NativeCodeLoader (NativeCodeLoader.java:&lt;clinit&gt;(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable2020-03-14 20:24:31,625 INFO [main] Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1129)) - session.id is deprecated. Instead, use dfs.metrics.session-id2020-03-14 20:24:31,626 INFO [main] jvm.JvmMetrics (JvmMetrics.java:init(76)) - Initializing JVM Metrics with processName=JobTracker, sessionId=2020-03-14 20:24:31,924 WARN [main] mapreduce.JobResourceUploader (JobResourceUploader.java:uploadFiles(64)) - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.2020-03-14 20:24:31,948 INFO [main] mapreduce.JobSubmitter (JobSubmitter.java:submitJobInternal(250)) - Cleaning up the staging area file:/var/sxt/hadoop/ha/mapred/staging/wangji241520974/.staging/job_local241520974_0001ExitCodeException exitCode=1: chmod: /private/var/sxt/hadoop/ha/mapred/staging/wangji241520974/.staging/job_local241520974_0001: No such file or directory 2. 解决方法在/private/var目录下创建sxt目录,并把权限改为777 12sudo mkdir sxtsudo chmod 777 sxt","categories":[{"name":"Mapreduce","slug":"Mapreduce","permalink":"https://wangjiosw.github.io/categories/Mapreduce/"}],"tags":[{"name":"macos","slug":"macos","permalink":"https://wangjiosw.github.io/tags/macos/"},{"name":"hadoop","slug":"hadoop","permalink":"https://wangjiosw.github.io/tags/hadoop/"}],"author":"wangji"},{"title":"胶囊间的动态路由-论文解读","slug":"deep-learning/capsule","date":"2020-03-01T05:25:00.000Z","updated":"2021-05-10T14:29:52.000Z","comments":true,"path":"2020/03/01/deep-learning/capsule/","link":"","permalink":"https://wangjiosw.github.io/2020/03/01/deep-learning/capsule/","excerpt":"","text":"1. Capsule介绍 Sabour, Sara, Nicholas Frosst, and Geoffrey E. Hinton. “Dynamic routing between capsules.” Advances in neural information processing systems. 2017. Capsule特色是“vector in vector out”，取代了以往的“scaler in scaler out”，也就是神经元的输入输出都变成了向量，从而算是对神经网络理论的一次革命。 然而在目前的深度学习中，从来不缺乏“vector in vector out”的案例，因此显然这不能算是Capsule的革命。比如在NLP中，一个词向量序列的输入模型，这个词向量序列再经过RNN/CNN/Attention的编码，输出一个新序列，不也是“vector in vector out”吗？ Capsule的革命在于：它提出了一种新的“vector in vector out”的传递方案，并且这种方案在很大程度上是可解释的。 深度学习（神经网络）为什么有效：神经网络通过层层叠加完成了对输入的层层抽象，这个过程某种程度上模拟了人的层次分类做法，从而完成对最终目标的输出，并且具有比较好的泛化能力。的确，神经网络应该是这样做的，然而它并不能告诉我们它确确实实是这样做的，这就是神经网络的难解释性，也就是很多人会将深度学习视为黑箱的原因之一。 下面介绍Capsule是怎么突破这一点的。 2. CapsNet模型CapsNet: 两个卷积层(Conv 1, PrimaryCaps)，一个全连接层(DigitCaps) 2.1 Conv1层常规的卷积层, 起像素级局部特征检测作用 $shape: [None,28,28,1] \\rightarrow [None,20,20,256]$ 2.2 PrimaryCaps层生成最低级卷积8D胶囊层（无路由） 胶囊：其实，只要把一个向量当作一个整体来看，它就是一个“胶囊”。可以这样理解：神经元就是标量，胶囊就是向量。Hinton的理解是：每一个胶囊表示一个属性，而胶囊的向量则表示这个属性的“标架”。也就是说，我们以前只是用一个标量表示有没有这个特征（比如有没有羽毛），现在我们用一个向量来表示，不仅仅表示有没有，还表示“有什么样的”（比如有什么颜色、什么纹理的羽毛），如果这样理解，就是说在对单个特征的表达上更丰富了。 简单的讲，PrimaryCaps层要输出一些8D的向量，每个向量代表一些比较低级的特征，向量的各个位置的值代表该特征的属性 PrimaryCaps层: 计算过程具有多种理解方式，其中之一为，8个并行的常规卷积层的叠堆 PrimaryCaps层的第一种理解方式8个并行的常规卷积层:卷积操作2参数:对Conv1层的输出进行8次卷积操作2所示的卷积操作： 然后对8个并行常规卷积层叠堆（对每个卷积层的各个通道在第四个维度上进行合并）： 8个[6,6,1,32]卷积层合并示意图如下:得到叠堆后的结果：[None,6,6,8,32]然后展开为[None,6x6x32,8,1]=[None,1152,8,1]，这样我们就得到了1152个初始胶囊，每个胶囊是一个纬度为[8,1]的向量，并代表某一特征 PrimaryCaps层的第二种理解方式 32个通道之间的卷积核是独立的(9x9大小) 8个并行卷积层之间的参数也是独立的 $Rightarrow$ 即共有8x32个大小为9x9的相互独立的卷积核，可看作8x32个通道的常规卷积和则可以用下面的操作得到和第一种理解方式相同的结果输出：$[None,6,6,32*8] \\overset{reshape}{\\rightarrow} [None,1152,8,1])$ 注意：虽然计算方式上与常规卷积层无差异，但意义上却已经大不相同！将达到8x1capsule的特征封装的效果 PrimaryCaps输出输出1152个8D的胶囊后，使用了一个Squash函数做非线性变换，那么，为什么要设计这个函数呢？这个函数的原理是啥 2.3 Squash函数为什么要设计squash函数：因为论文希望Capsule能有一个性质：胶囊的模长能够代表这个特征的概率，即特征的“显著程度”， 模长越大，这个特征越显著，而我们又希望有一个有界的指标来对这个“显著程度”进行衡量，所以就只能对这个模长进行压缩了 squash函数的原理：$$squash(x) = \\frac{||x||^2}{1+||x||^2} \\frac{x}{||x||}$$ $\\frac{x}{||x||}$: 将x的模长变为1 $\\frac{||x||^2}{1+||x||^2}$: 起缩放x的模长的作用，x模长越大，$\\frac{||x||^2}{1+||x||^2}$越趋近于1，||x||=0时，$\\frac{||x||^2}{1+||x||^2}=0$ 则$y=squash(x)$的效果为：x的模长越大，y的模长越趋近于1 PrimaryCaps输出的1152个8D的胶囊经过squash函数后非线性变换后，都具有了胶囊的模长能够代表这个特征的概率的特性，这些新的胶囊接着作为DigitCaps层的输入。 2.4 DigitCaps层由于该层解释起来比较复杂，所以先从简单例子开始，慢慢推出该层的流程及原理。 capsule示意capsule示意图: 如上图所示，底层的胶囊和高层的胶囊构成一些连接关系那么，这些胶囊要怎么运算，才能体现出“层层抽象”、“层层分类”的特性呢？让我们先看其中一部分连接： u,v都是胶囊，图上只展示了$u_1$的连接。这也就是说，目前已经有了$u_1$这个特征（假设是羽毛），那么我想知道它属于上层特征$v_1,v_2 ,v_3,v_4$（假设分别代表了鸡、鸭、鱼、狗）中的哪一个。分类问题我们显然已经是很熟悉了，不就是内积后softmax吗？于是单靠$u_1$这个特征，我们推导出它是属于鸡、鸭、鱼、狗的概率分别是: $$(p_{1|1},p_{2|1},p_{3|1},p_{4|1})=\\frac{1}{Z_1} (e^{&lt;u_1,v_1&gt;},e^{&lt;u_1,v_2&gt;},e^{&lt;u_1,v_3&gt;},e^{&lt;u_1,v_4&gt;})$$ 我们期望$p_{1|1}，p_{2|1}$会明显大于$p_{3|1}，p_{4|1}$（鸡鸭有羽毛，鱼狗没羽毛）不过，单靠这个特征还不够，我们还需要综合各个特征，于是可以把上述操作对各个u_i都做一遍，继而得到$$(p_{1|2},p_{2|2},p_{3|2},p_{4|2}), (p_{1|3},p_{2|3},p_{3|3},p_{4|3}), …$$ 问题是，现在得到这么多预测结果，究竟要选择哪个呢？而且又不是真的要做分类，我们要的是融合这些特征，构成更高级的特征。 于是Hinton认为，既然$u_i$这个特征得到的概率分布是$(p_{1|i},p_{2|i},p_{3|i},p_{4|i})$那么我把这个特征切成四份，分别为$(p_{1|i}u_i,p_{2|i}u_i,p_{3|i}u_i,p_{4|i}u_i)$, 然后把这几个特征分别传给$v_1,v_2,v_3,v_4$，最后$v_1,v_2,v_3,v_4$其实就是底层传入的特征的累加 $$v_j=squash(p_{j|i} u_i )=squash(\\sum_{i} \\frac{e^{&lt;u_i,v_j&gt;}}{Z_i} u_i)$$ 从上往下看，那么Capsule就是每个底层特征分别做分类，然后将分类结果整合。这时$v_j$应该尽量与所有$u_i$都比较靠近，靠近的度量是内积。因此，从下往上看的话，可以认为$v_j$实际上就是各个$u_i$的某个聚类中心，而Capsule的核心思想就是输出是输入的某种聚类结果。 动态路由：注意到式子$v_j=squash(\\sum_{i} \\frac{e^{&lt;u_i,v_j&gt;}}{Z_i} u_i)$，为了求$v_j$需要求softmax，可是为了求softmax又需要知道$v_j$，这不是个鸡生蛋、蛋生鸡的问题了吗？而“动态路由”正是为了解决这一问题而提出的，它能够根据自身的特性来更新（部分）参数，从而初步达到了Hinton的放弃梯度下降的目标 下面通过几个例子来解释动态路由的过程： 例1:让我们先回到普通的神经网络，大家知道，激活函数在神经网络中的地位是举足轻重的。当然，激活函数本身很简单，比如一个tanh激活的全连接层。 可是，如果想用$x=y+cos⁡y$的反函数来激活呢？也就是说，得解出$y=f(x)$，然后再用它来做激活函数。然而$x=y+cos⁡y$的反函数是一个超越函数，也就是不可能用初等函数有限地表示出来。但我们可以通过迭代法求出y：$$y_{n+1} = x - cos y_n$$ 选择$y_0=x$，代入上式迭代几次，基本上就可以得到比较准确的y了。假如迭代3次，那就是$$y=x-cos⁡(x-cos⁡(x-cos⁡x))$$可以发现这和动态路由的过程有点像 例2:再来看一个例子，这个例子可能在NLP中有很多对应的情景，但图像领域其实也不少。考虑一个向量序列$(x_1,x_2,…,x_n)$，我现在要想办法将这n个向量整合成一个向量x（encoder），然后用这个向量来做分类：$$x= \\sum_{i=1}^n \\lambda _i x_i$$这里的λ_i相当于衡量了x与x_i的相似度。那么，在x出现之前，凭什么能够确定这个相似度呢？解决这个问题的一个方案也是迭代。首先我们也可以定义一个基于softmax的相似度指标，然后让 $$x= \\sum_{i=1}^n \\frac{e^{&lt;x,x_i&gt;}}{Z} x_i$$ 一开始，我们一无所知，所以只好取x为各个$x_i$的均值，然后代入右边就可以算出一个x，再把它代入右边，反复迭代就行，一般迭代有限次就可以收敛，于是就可以将这个迭代过程嵌入到神经网络中了。 如果说例1跟动态路由只是神似，那么例2已经跟动态路由是神似＋形似了。 通过例1，例2，已经可以很清晰的开始解释动态路由过程了为了得到各个v_j，一开始先让它们全都等于u_i的均值，然后反复迭代就好。说白了，输出是输入的聚类结果，而聚类通常都需要迭代算法，这个迭代算法就称为“动态路由”。 到此，就可以写出论文里的动态路由的算法了： 动态路由算法初始化$b_{ij}$=0迭代r次：&nbsp;&nbsp;&nbsp;&nbsp;$c_i \\leftarrow softmax(b_i)$&nbsp;&nbsp;&nbsp;&nbsp;$s_j \\leftarrow \\sum_i c_{ij} u_i$&nbsp;&nbsp;&nbsp;&nbsp;$v_j \\leftarrow squash(s_j)$&nbsp;&nbsp;&nbsp;&nbsp;$b_{ij} \\leftarrow b_{ij} + &lt;u_i,v_j&gt;$返回$v_j$ 这里的$c_{ij}$就是前文的$p_{j|i}$前面已经说了，$v_j$是作为输入$u_i$的某种聚类中心出现的，而从不同角度看输入，得到的聚类结果显然是不一样的。那么为了实现“多角度看特征”，于是可以在每个胶囊传入下一个胶囊之前，都要先乘上一个矩阵做变换，所以式$v_j=squash(\\sum_{i} \\frac{e^{&lt;u_i,v_j&gt;}}{Z_i} u_i)$实际上应该要变为 $v_j=squash(\\sum_{i} \\frac{e^{&lt;u_i,v_j&gt;}}{Z_i} \\hat{u} _{j|i})$ $\\hat{u}{j|i} = W{ji} u_i$ 这里的$W_{ji}$是待训练的矩阵，这里的乘法是矩阵乘法，也就是矩阵乘以向量。所以，Capsule变成了下图这时候就可以得到完整动态路由了: 动态路由算法初始化$b_{ij}$=0迭代r次：&nbsp;&nbsp;&nbsp;&nbsp;$c_i \\leftarrow softmax(b_i)$&nbsp;&nbsp;&nbsp;&nbsp;$s_j \\leftarrow \\sum_i c_{ij} \\hat{u}{j|i}$&nbsp;&nbsp;&nbsp;&nbsp;$v_j \\leftarrow squash(s_j)$&nbsp;&nbsp;&nbsp;&nbsp;$b{ij} \\leftarrow b_{ij} + &lt;u_i,v_j&gt;$返回$v_j$ 这样的Capsule层，显然相当于普通神经网络中的全连接层。 DigitCaps层流程总结 将PrimaryCaps输入的1152个8D的胶囊从乘$W_{ji}$，以达到不同角度看输入的目的，得到[None,10,1152,16,1] 对每个1152里面的16D的胶囊，通过动态路由算法聚类出一个$v_j$ 返回[None,10,16],即输出10个胶囊，分别对应数字0～9，胶囊的模长代表是该数字的概率，每个胶囊内部的值代表了该数字的某一属性 2.5 重构层重构层就比较简单了，但是也有一些细节需要说明一下 网络很简单，就是三个全连接层，其中有个masked操作，具体原理如下：因为DigitCaps输出[None,10,16]，即每个样本输出10个16D的胶囊，胶囊的模长代表图片是这个类别的概率，而每个16D的胶囊里面各个位置的值则代表了这个数字的一系列属性，重构是该胶囊已经包含了大部分的信息，假设要重构的是数字5，那么就把DigitCaps该位置的mask设置为1，其他位置为0，那么实际重构事，就只有这个胶囊的信息参与了运算。 2.6 损失函数Margin loss + recon lossMargin loss函数：$$L_c = T_c max⁡(0,m^+ - ||v_c ||)^2+ \\lambda (1-T_c )max⁡(0,||v_c||-m^- )^2$$ $c$:类别 $T_c$:指示函数（分类c存在为1，否则为0） $m^-$:$||v_c ||$上边界，避免假阴性，遗漏实际预测到存在的分类的情况 $m^+$:$||v_c |)|$下边界，避免假阳性 margin loss: $\\sum_c L_c$ 重构误差： 作用： 正则化 重构网络： MLP 重构误差计算方式MSE 3. 运行结果3.1测试集分类结果 Your browser does not support the video tag. 论文分类结果: 复现（routing:3,Reconstruction:yes）结果在测试集的准确率平均可达99.24%以上，基本复现成功 3.2 单数字重构效果论文单数字重构效果: 复现单数字重构效果: 3.3 重叠数字重构效果论文重叠数字重构效果: 复现重叠数字重构效果:第一行为实际图片和标签，第二行为预测的图片和标签，第三四行是把第二行两个图片分开的结果 4. github源码地址 https://github.com/wangjiosw/capsule-pytorch 5. 参考文章[1] 揭开迷雾，来一顿美味的Capsule盛宴 [2] 再来一顿贺岁宴：从K-Means到Capsule","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://wangjiosw.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"cpasule","slug":"cpasule","permalink":"https://wangjiosw.github.io/tags/cpasule/"},{"name":"paper","slug":"paper","permalink":"https://wangjiosw.github.io/tags/paper/"}],"author":"wangji"},{"title":"Mac OS VMware Fusion Centos6.5虚拟机网络设置","slug":"bigdata/environment/vmware","date":"2020-02-29T15:25:00.000Z","updated":"2021-05-10T14:29:52.000Z","comments":true,"path":"2020/02/29/bigdata/environment/vmware/","link":"","permalink":"https://wangjiosw.github.io/2020/02/29/bigdata/environment/vmware/","excerpt":"","text":"1. 安装vmware虚拟机安装vmware虚拟机，并新建一个centos 64位的虚拟机 2. 设置虚拟机网络模式 3. 查看vmware的网关和掩码在Mac电脑的终端输入： 1cat /Library/Preferences/VMware\\ Fusion/vmnet8/nat.conf 输出结果如下：这里的ip和netmask即为vmware虚拟机的网关和掩码 123# NAT gateway addressip = 172.16.143.2netmask = 255.255.255.0 ip和netmask后面配置centos虚拟机的网络时分别对于网关和掩码。 4. 配置centos虚拟机的网络在centos虚拟机的终端输入： 1vi /etc/sysconfig/network-scripts/ifcfg-eth0 然后： 删除UUID和MAC地址 ONBOOT=yes BOOTPROTO=static IPADDR=172.16.143.101 NETMASK=255.255.255.0 GATEWAY=172.16.143.2 DNS1=172.16.143.2 保存并退出，然后在centos虚拟机的终端输入： 1service network restart ping 一下百度看是否能ping通： 1ping www.baidu.com","categories":[{"name":"虚拟机","slug":"虚拟机","permalink":"https://wangjiosw.github.io/categories/%E8%99%9A%E6%8B%9F%E6%9C%BA/"}],"tags":[{"name":"vmware","slug":"vmware","permalink":"https://wangjiosw.github.io/tags/vmware/"},{"name":"macos","slug":"macos","permalink":"https://wangjiosw.github.io/tags/macos/"}],"author":"wangji"},{"title":"Torchtext使用教程","slug":"deep-learning/torchtext_use","date":"2020-02-29T13:25:00.000Z","updated":"2021-05-10T14:29:52.000Z","comments":true,"path":"2020/02/29/deep-learning/torchtext_use/","link":"","permalink":"https://wangjiosw.github.io/2020/02/29/deep-learning/torchtext_use/","excerpt":"","text":"Torchtext使用教程主要内容： 如何使用torchtext建立语料库 如何使用torchtext将词转下标，下标转词，词转词向量 如何建立相应的迭代器 torchtext预处理流程： 定义Field：声明如何处理数据 定义Dataset：得到数据集，此时数据集里每一个样本是一个 经过 Field声明的预处理 预处理后的 wordlist 建立vocab：在这一步建立词汇表，词向量(word embeddings) 构造迭代器：构造迭代器，用来分批次训练模型 1. 下载数据：kaggle：Movie Review Sentiment Analysis (Kernels Only)train.tsv contains the phrases and their associated sentiment labels. We have additionally provided a SentenceId so that you can track which phrases belong to a single sentence. test.tsv contains just phrases. You must assign a sentiment label to each phrase. The sentiment labels are:0 - negative1 - somewhat negative2 - neutral3 - somewhat positive4 - positive 下载得到：train.tsv和test.tsv 读取文件，查看文件123import pandas as pddata = pd.read_csv(&#x27;train.tsv&#x27;, sep=&#x27;\\t&#x27;)test = pd.read_csv(&#x27;test.tsv&#x27;, sep=&#x27;\\t&#x27;) train.tsv1data[:5] test.tsv1test[:5] 2. 划分验证集123456from sklearn.model_selection import train_test_split# create train and validation set train, val = train_test_split(data, test_size=0.2)train.to_csv(&quot;train.csv&quot;, index=False)val.to_csv(&quot;val.csv&quot;, index=False) 3. 定义Field首先导入需要的包和定义pytorch张量使用的DEVICE 1234567import spacyimport torchfrom torchtext import data, datasetsfrom torchtext.vocab import Vectorsfrom torch.nn import initDEVICE = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) Torchtext采用了一种声明式的方法来加载数据：你来告诉Torchtext你希望的数据是什么样子的，剩下的由torchtext来处理。实现这种声明的是Field，Field确定了一种你想要怎么去处理数据。data.Field(…) Field的参数如下： sequential: Whether the datatype represents sequential data. If False, no tokenization is applied. Default: True. use_vocab: Whether to use a Vocab object. If False, the data in this field should already be numerical. Default: True. init_token: A token that will be prepended to every example using this field, or None for no initial token. Default: None. eos_token: A token that will be appended to every example using this field, or None for no end-of-sentence token. Default: None. fix_length: A fixed length that all examples using this field will be padded to, or None for flexible sequence lengths. Default: None. dtype: The torch.dtype class that represents a batch of examples of this kind of data. Default: torch.long. preprocessing: The Pipeline that will be applied to examples using this field after tokenizing but before numericalizing. Many Datasets replace this attribute with a custom preprocessor. Default: None. postprocessing: A Pipeline that will be applied to examples using this field after numericalizing but before the numbers are turned into a Tensor. The pipeline function takes the batch as a list, and the field’s Vocab. Default: None. lower: Whether to lowercase the text in this field. Default: False. tokenize: The function used to tokenize strings using this field into sequential examples. If “spacy”, the SpaCy tokenizer is used. If a non-serializable function is passed as an argument, the field will not be able to be serialized. Default: string.split. tokenizer_language: The language of the tokenizer to be constructed. Various languages currently supported only in SpaCy. include_lengths: Whether to return a tuple of a padded minibatch and a list containing the lengths of each examples, or just a padded minibatch. Default: False. batch_first: Whether to produce tensors with the batch dimension first. Default: False. pad_token: The string token used as padding. Default: ““. unk_token: The string token used to represent OOV words. Default: ““. pad_first: Do the padding of the sequence at the beginning. Default: False. truncate_first: Do the truncating of the sequence at the beginning. Default: False stop_words: Tokens to discard during the preprocessing step. Default: None is_target: Whether this field is a target variable. Affects iteration over batches. Default: False 例： 1234567891011121314151617spacy_en = spacy.load(&#x27;en&#x27;)def tokenizer(text): # create a tokenizer function &quot;&quot;&quot; 定义分词操作 &quot;&quot;&quot; return [tok.text for tok in spacy_en.tokenizer(text)]&quot;&quot;&quot;field在默认的情况下都期望一个输入是一组单词的序列，并且将单词映射成整数。这个映射被称为vocab。如果一个field已经被数字化了并且不需要被序列化，可以将参数设置为use_vocab=False以及sequential=False。&quot;&quot;&quot;LABEL = data.Field(sequential=False, use_vocab=False)TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True) 4. 定义DatasetThe fields知道当给定原始数据的时候要做什么。现在，我们需要告诉fields它需要处理什么样的数据。这个功能利用Datasets来实现。 Torchtext有大量内置的Datasets去处理各种数据格式。 TabularDataset官网介绍: Defines a Dataset of columns stored in CSV, TSV, or JSON format. 对于csv/tsv类型的文件，TabularDataset很容易进行处理，故我们选它来生成Dataset 1234567891011&quot;&quot;&quot;我们不需要 &#x27;PhraseId&#x27; 和 &#x27;SentenceId&#x27;这两列, 所以我们给他们的field传递 None如果你的数据有列名，如我们这里的&#x27;Phrase&#x27;,&#x27;Sentiment&#x27;,...设置skip_header=True,不然它会把列名也当一个数据处理&quot;&quot;&quot;train,val = data.TabularDataset.splits( path=&#x27;.&#x27;, train=&#x27;train.csv&#x27;,validation=&#x27;val.csv&#x27;, format=&#x27;csv&#x27;,skip_header=True, fields=[(&#x27;PhraseId&#x27;,None),(&#x27;SentenceId&#x27;,None),(&#x27;Phrase&#x27;, TEXT), (&#x27;Sentiment&#x27;, LABEL)])test = data.TabularDataset(&#x27;test.tsv&#x27;, format=&#x27;tsv&#x27;,skip_header=True, fields=[(&#x27;PhraseId&#x27;,None),(&#x27;SentenceId&#x27;,None),(&#x27;Phrase&#x27;, TEXT)]) 注意：传入的(name, field)必须与列的顺序相同。 查看生成的dataset： 123print(train[5])print(train[5].__dict__.keys())print(train[5].Phrase,train[0].Sentiment) 输出： 5. 建立vocab我们可以看到第6行的输入，它是一个Example对象。Example对象绑定了一行中的所有属性，可以看到，句子已经被分词了，但是没有转化为数字。 这是因为我们还没有建立vocab，我们将在下一步建立vocab。 Torchtext可以将词转化为数字，但是它需要被告知需要被处理的全部范围的词。我们可以用下面这行代码： 123TEXT.build_vocab(train, vectors=&#x27;glove.6B.100d&#x27;)#, max_size=30000)# 当 corpus 中有的 token 在 vectors 中不存在时 的初始化方式.TEXT.vocab.vectors.unk_init = init.xavier_uniform 这行代码使得 Torchtext遍历训练集中的绑定TEXT field的数据，将单词注册到vocabulary，并自动构建embedding矩阵。 ‘glove.6B.100d’ 为torchtext支持的词向量名字，第一次使用是会自动下载并保存在当前目录的 .vector_cache里面。 torchtext支持的词向量 charngram.100d fasttext.en.300d fasttext.simple.300d glove.42B.300d glove.840B.300d glove.twitter.27B.25d glove.twitter.27B.50d glove.twitter.27B.100d glove.twitter.27B.200d glove.6B.50d glove.6B.100d glove.6B.200d glove.6B.300d 例： 如果打算使用fasttext.en.300d词向量，只需把上面的代码里的vector=’…’里面的词向量名字换一下即可，具体如下： 1TEXT.build_vocab(train, vectors=&#x27;fasttext.en.300d&#x27;) 到这一步，我们已经可以把词转为数字，数字转为词，词转为词向量了 1234567print(TEXT.vocab.itos[1510])print(TEXT.vocab.stoi[&#x27;bore&#x27;])# 词向量矩阵: TEXT.vocab.vectorsprint(TEXT.vocab.vectors.shape)word_vec = TEXT.vocab.vectors[TEXT.vocab.stoi[&#x27;bore&#x27;]]print(word_vec.shape)print(word_vec) 输出： 6. 构造迭代器我们日常使用pytorch训练网络时，每次训练都是输入一个batch，那么，我们怎么把前面得到的dataset转为迭代器，然后遍历迭代器获取batch输入呢？下面将介绍torchtext时怎么实现这一功能的。 和Dataset一样，torchtext有大量内置的迭代器，我们这里选择的是BucketIterator，官网对它的介绍如下： Defines an iterator that batches examples of similar lengths together. Minimizes amount of padding needed while producing freshly shuffled batches for each new epoch. 123456789train_iter = data.BucketIterator(train, batch_size=128, sort_key=lambda x: len(x.Phrase), shuffle=True,device=DEVICE)val_iter = data.BucketIterator(val, batch_size=128, sort_key=lambda x: len(x.Phrase), shuffle=True,device=DEVICE)# 在 test_iter , sort一定要设置成 False, 要不然会被 torchtext 搞乱样本顺序test_iter = data.Iterator(dataset=test, batch_size=128, train=False, sort=False, device=DEVICE) 迭代器使用方法一12345batch = next(iter(train_iter))data = batch.Phraselabel = batch.Sentimentprint(batch.Phrase.shape)print(batch.Phrase) 输出结果：可以发现，它输出的是word index，后面的128是batch size 方法二123for batch in train_iter: data = batch.Phrase label = batch.Sentiment 7. 完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129import spacyimport torchfrom torchtext import data, datasetsfrom torchtext.vocab import Vectorsfrom torch.nn import initimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimimport numpy as npfrom sklearn.model_selection import train_test_splitimport pandas as pdDEVICE = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)data = pd.read_csv(&#x27;train.tsv&#x27;, sep=&#x27;\\t&#x27;)test = pd.read_csv(&#x27;test.tsv&#x27;, sep=&#x27;\\t&#x27;)# create train and validation set train, val = train_test_split(data, test_size=0.2)train.to_csv(&quot;train.csv&quot;, index=False)val.to_csv(&quot;val.csv&quot;, index=False)spacy_en = spacy.load(&#x27;en&#x27;)def tokenizer(text): # create a tokenizer function return [tok.text for tok in spacy_en.tokenizer(text)]# FieldTEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True)LABEL = data.Field(sequential=False, use_vocab=False)# Datasettrain,val = data.TabularDataset.splits( path=&#x27;.&#x27;, train=&#x27;train.csv&#x27;,validation=&#x27;val.csv&#x27;, format=&#x27;csv&#x27;,skip_header=True, fields=[(&#x27;PhraseId&#x27;,None),(&#x27;SentenceId&#x27;,None),(&#x27;Phrase&#x27;, TEXT), (&#x27;Sentiment&#x27;, LABEL)])test = data.TabularDataset(&#x27;test.tsv&#x27;, format=&#x27;tsv&#x27;,skip_header=True, fields=[(&#x27;PhraseId&#x27;,None),(&#x27;SentenceId&#x27;,None),(&#x27;Phrase&#x27;, TEXT)])# build vocabTEXT.build_vocab(train, vectors=&#x27;glove.6B.100d&#x27;)#, max_size=30000)TEXT.vocab.vectors.unk_init = init.xavier_uniform# Iteratortrain_iter = data.BucketIterator(train, batch_size=128, sort_key=lambda x: len(x.Phrase), shuffle=True,device=DEVICE)val_iter = data.BucketIterator(val, batch_size=128, sort_key=lambda x: len(x.Phrase), shuffle=True,device=DEVICE)# 在 test_iter , sort一定要设置成 False, 要不然会被 torchtext 搞乱样本顺序test_iter = data.Iterator(dataset=test, batch_size=128, train=False, sort=False, device=DEVICE)&quot;&quot;&quot;由于目的是学习torchtext的使用，所以只定义了一个简单模型&quot;&quot;&quot;len_vocab = len(TEXT.vocab)class Enet(nn.Module): def __init__(self): super(Enet, self).__init__() self.embedding = nn.Embedding(len_vocab,100) self.lstm = nn.LSTM(100,128,3,batch_first=True)#,bidirectional=True) self.linear = nn.Linear(128,5) def forward(self, x): batch_size,seq_num = x.shape vec = self.embedding(x) out, (hn, cn) = self.lstm(vec) out = self.linear(out[:,-1,:]) out = F.softmax(out,-1) return outmodel = Enet()&quot;&quot;&quot;将前面生成的词向量矩阵拷贝到模型的embedding层这样就自动的可以将输入的word index转为词向量&quot;&quot;&quot;model.embedding.weight.data.copy_(TEXT.vocab.vectors) model.to(DEVICE)# 训练optimizer = optim.Adam(model.parameters())#,lr=0.000001)n_epoch = 20best_val_acc = 0for epoch in range(n_epoch): for batch_idx, batch in enumerate(train_iter): data = batch.Phrase target = batch.Sentiment target = torch.sparse.torch.eye(5).index_select(dim=0, index=target.cpu().data) target = target.to(DEVICE) data = data.permute(1,0) optimizer.zero_grad() out = model(data) loss = -target*torch.log(out)-(1-target)*torch.log(1-out) loss = loss.sum(-1).mean() loss.backward() optimizer.step() if (batch_idx+1) %200 == 0: _,y_pre = torch.max(out,-1) acc = torch.mean((torch.tensor(y_pre == batch.Sentiment,dtype=torch.float))) print(&#x27;epoch: %d \\t batch_idx : %d \\t loss: %.4f \\t train acc: %.4f&#x27; %(epoch,batch_idx,loss,acc)) val_accs = [] for batch_idx, batch in enumerate(val_iter): data = batch.Phrase target = batch.Sentiment target = torch.sparse.torch.eye(5).index_select(dim=0, index=target.cpu().data) target = target.to(DEVICE) data = data.permute(1,0) out = model(data) _,y_pre = torch.max(out,-1) acc = torch.mean((torch.tensor(y_pre == batch.Sentiment,dtype=torch.float))) val_accs.append(acc) acc = np.array(val_accs).mean() if acc &gt; best_val_acc: print(&#x27;val acc : %.4f &gt; %.4f saving model&#x27;%(acc,best_val_acc)) torch.save(model.state_dict(), &#x27;params.pkl&#x27;) best_val_acc = acc print(&#x27;val acc: %.4f&#x27;%(acc)) 8. 参考 pytorch学习笔记—Torchtext 使用 torchtext 做 Toxic Comment Classification 比赛的数据预处理 How to use TorchText for neural machine translation, plus hack to make it 5x faster","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://wangjiosw.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"nlp","slug":"nlp","permalink":"https://wangjiosw.github.io/tags/nlp/"},{"name":"torchtext","slug":"torchtext","permalink":"https://wangjiosw.github.io/tags/torchtext/"},{"name":"python","slug":"python","permalink":"https://wangjiosw.github.io/tags/python/"}]}],"categories":[{"name":"开发工具","slug":"开发工具","permalink":"https://wangjiosw.github.io/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"},{"name":"Docker","slug":"Docker","permalink":"https://wangjiosw.github.io/categories/Docker/"},{"name":"Java设计模式","slug":"Java设计模式","permalink":"https://wangjiosw.github.io/categories/Java%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"Spark","slug":"Spark","permalink":"https://wangjiosw.github.io/categories/Spark/"},{"name":"Kafka","slug":"Kafka","permalink":"https://wangjiosw.github.io/categories/Kafka/"},{"name":"Hive","slug":"Hive","permalink":"https://wangjiosw.github.io/categories/Hive/"},{"name":"Mapreduce","slug":"Mapreduce","permalink":"https://wangjiosw.github.io/categories/Mapreduce/"},{"name":"JVM","slug":"JVM","permalink":"https://wangjiosw.github.io/categories/JVM/"},{"name":"深度学习","slug":"深度学习","permalink":"https://wangjiosw.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"Hbase","slug":"Hbase","permalink":"https://wangjiosw.github.io/categories/Hbase/"},{"name":"虚拟机","slug":"虚拟机","permalink":"https://wangjiosw.github.io/categories/%E8%99%9A%E6%8B%9F%E6%9C%BA/"}],"tags":[{"name":"java","slug":"java","permalink":"https://wangjiosw.github.io/tags/java/"},{"name":"idea","slug":"idea","permalink":"https://wangjiosw.github.io/tags/idea/"},{"name":"dbeaver","slug":"dbeaver","permalink":"https://wangjiosw.github.io/tags/dbeaver/"},{"name":"hive","slug":"hive","permalink":"https://wangjiosw.github.io/tags/hive/"},{"name":"spark","slug":"spark","permalink":"https://wangjiosw.github.io/tags/spark/"},{"name":"kafka","slug":"kafka","permalink":"https://wangjiosw.github.io/tags/kafka/"},{"name":"protoType","slug":"protoType","permalink":"https://wangjiosw.github.io/tags/protoType/"},{"name":"function","slug":"function","permalink":"https://wangjiosw.github.io/tags/function/"},{"name":"parittion","slug":"parittion","permalink":"https://wangjiosw.github.io/tags/parittion/"},{"name":"topic","slug":"topic","permalink":"https://wangjiosw.github.io/tags/topic/"},{"name":"partition","slug":"partition","permalink":"https://wangjiosw.github.io/tags/partition/"},{"name":"macos","slug":"macos","permalink":"https://wangjiosw.github.io/tags/macos/"},{"name":"sougou","slug":"sougou","permalink":"https://wangjiosw.github.io/tags/sougou/"},{"name":"hadoop jar","slug":"hadoop-jar","permalink":"https://wangjiosw.github.io/tags/hadoop-jar/"},{"name":"OutOfMemoryError","slug":"OutOfMemoryError","permalink":"https://wangjiosw.github.io/tags/OutOfMemoryError/"},{"name":"heap","slug":"heap","permalink":"https://wangjiosw.github.io/tags/heap/"},{"name":"pytorch","slug":"pytorch","permalink":"https://wangjiosw.github.io/tags/pytorch/"},{"name":"RNN","slug":"RNN","permalink":"https://wangjiosw.github.io/tags/RNN/"},{"name":"GRU","slug":"GRU","permalink":"https://wangjiosw.github.io/tags/GRU/"},{"name":"LSTM","slug":"LSTM","permalink":"https://wangjiosw.github.io/tags/LSTM/"},{"name":"矩阵求导","slug":"矩阵求导","permalink":"https://wangjiosw.github.io/tags/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/"},{"name":"ntpdate","slug":"ntpdate","permalink":"https://wangjiosw.github.io/tags/ntpdate/"},{"name":"jline","slug":"jline","permalink":"https://wangjiosw.github.io/tags/jline/"},{"name":"hadoop","slug":"hadoop","permalink":"https://wangjiosw.github.io/tags/hadoop/"},{"name":"cpasule","slug":"cpasule","permalink":"https://wangjiosw.github.io/tags/cpasule/"},{"name":"paper","slug":"paper","permalink":"https://wangjiosw.github.io/tags/paper/"},{"name":"vmware","slug":"vmware","permalink":"https://wangjiosw.github.io/tags/vmware/"},{"name":"nlp","slug":"nlp","permalink":"https://wangjiosw.github.io/tags/nlp/"},{"name":"torchtext","slug":"torchtext","permalink":"https://wangjiosw.github.io/tags/torchtext/"},{"name":"python","slug":"python","permalink":"https://wangjiosw.github.io/tags/python/"}]}