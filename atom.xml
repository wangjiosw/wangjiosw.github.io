<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Technology is a means, not an end</title>
  
  
  <link href="https://wangjiosw.github.io/atom.xml" rel="self"/>
  
  <link href="https://wangjiosw.github.io/"/>
  <updated>2021-05-10T14:29:52.000Z</updated>
  <id>https://wangjiosw.github.io/</id>
  
  <author>
    <name>wangji</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Idea No usages found in Project and Libraries</title>
    <link href="https://wangjiosw.github.io/2021/04/26/idea/idea-usage-not-find/"/>
    <id>https://wangjiosw.github.io/2021/04/26/idea/idea-usage-not-find/</id>
    <published>2021-04-26T01:28:00.000Z</published>
    <updated>2021-05-10T14:29:52.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h1><p>find usage 时，方法存在引用，结果却出现”No usages found in Project and Librarie”</p><h1 id="2-解决方法"><a href="#2-解决方法" class="headerlink" title="2. 解决方法"></a>2. 解决方法</h1><p>Idea : File –&gt; Invalidate Caches –&gt; Invalidate and Restart</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;1-问题描述&quot;&gt;&lt;a href=&quot;#1-问题描述&quot; class=&quot;headerlink&quot; title=&quot;1. 问题描述&quot;&gt;&lt;/a&gt;1. 问题描述&lt;/h1&gt;&lt;p&gt;find usage 时，方法存在引用，结果却出现”No usages found in Project</summary>
      
    
    
    
    <category term="开发工具" scheme="https://wangjiosw.github.io/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="java" scheme="https://wangjiosw.github.io/tags/java/"/>
    
    <category term="idea" scheme="https://wangjiosw.github.io/tags/idea/"/>
    
  </entry>
  
  <entry>
    <title>DBeaver执行SELECT语句报错&quot;com/google/common/primitives/Ints&quot;</title>
    <link href="https://wangjiosw.github.io/2021/03/15/bigdata/hive/dbeaver-select-error/"/>
    <id>https://wangjiosw.github.io/2021/03/15/bigdata/hive/dbeaver-select-error/</id>
    <published>2021-03-15T07:14:00.000Z</published>
    <updated>2021-05-10T14:29:52.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h1><p>使用Dbeaver执行SELECT语句查询数据时报以下错误</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">org.jkiss.dbeaver.model.exec.DBCException: com/google/common/primitives/Ints</span><br><span class="line">at org.jkiss.dbeaver.ui.editors.sql.execute.SQLQueryJob.extractData(SQLQueryJob.java:809)</span><br><span class="line">at org.jkiss.dbeaver.ui.editors.sql.SQLEditor$QueryResultsContainer.readData(SQLEditor.java:3008)</span><br><span class="line">at org.jkiss.dbeaver.ui.controls.resultset.ResultSetJobDataRead.lambda$0(ResultSetJobDataRead.java:120)</span><br><span class="line">at org.jkiss.dbeaver.model.exec.DBExecUtils.tryExecuteRecover(DBExecUtils.java:168)</span><br><span class="line">at org.jkiss.dbeaver.ui.controls.resultset.ResultSetJobDataRead.run(ResultSetJobDataRead.java:118)</span><br><span class="line">at org.jkiss.dbeaver.ui.controls.resultset.ResultSetViewer$ResultSetDataPumpJob.run(ResultSetViewer.java:4425)</span><br><span class="line">at org.jkiss.dbeaver.model.runtime.AbstractJob.run(AbstractJob.java:105)</span><br><span class="line">at org.eclipse.core.internal.jobs.Worker.run(Worker.java:63)</span><br><span class="line">Caused by: org.jkiss.dbeaver.DBException: com/google/common/primitives/Ints</span><br><span class="line">at org.jkiss.dbeaver.model.exec.DBExecUtils.tryExecuteRecover(DBExecUtils.java:227)</span><br><span class="line">at org.jkiss.dbeaver.ui.editors.sql.execute.SQLQueryJob.executeSingleQuery(SQLQueryJob.java:423)</span><br><span class="line">at org.jkiss.dbeaver.ui.editors.sql.execute.SQLQueryJob.extractData(SQLQueryJob.java:804)</span><br><span class="line">... 7 more</span><br><span class="line">Caused by: java.lang.NoClassDefFoundError: com/google/common/primitives/Ints</span><br><span class="line">at org.apache.hive.service.cli.Column.&lt;init&gt;(Column.java:149)</span><br><span class="line">at org.apache.hive.service.cli.ColumnBasedSet.&lt;init&gt;(ColumnBasedSet.java:52)</span><br><span class="line">at org.apache.hive.service.cli.RowSetFactory.create(RowSetFactory.java:37)</span><br><span class="line">at org.apache.hive.jdbc.HiveQueryResultSet.fetchNextResultset(HiveQueryResultSet.java:446)</span><br><span class="line">at org.apache.hive.jdbc.HiveQueryResultSet.next(HiveQueryResultSet.java:519)</span><br><span class="line">at org.jkiss.dbeaver.model.impl.jdbc.exec.JDBCResultSetImpl.next(JDBCResultSetImpl.java:272)</span><br><span class="line">at org.jkiss.dbeaver.model.impl.jdbc.exec.JDBCResultSetImpl.nextRow(JDBCResultSetImpl.java:180)</span><br><span class="line">at org.jkiss.dbeaver.ui.editors.sql.execute.SQLQueryJob.fetchQueryData(SQLQueryJob.java:708)</span><br><span class="line">at org.jkiss.dbeaver.ui.editors.sql.execute.SQLQueryJob.executeStatement(SQLQueryJob.java:536)</span><br><span class="line">at org.jkiss.dbeaver.ui.editors.sql.execute.SQLQueryJob.lambda$0(SQLQueryJob.java:436)</span><br><span class="line">at org.jkiss.dbeaver.model.exec.DBExecUtils.tryExecuteRecover(DBExecUtils.java:168)</span><br><span class="line">... 9 more</span><br></pre></td></tr></table></figure><h1 id="2-解决方法"><a href="#2-解决方法" class="headerlink" title="2.解决方法"></a>2.解决方法</h1><p>在DBeaver的hive连接的驱动设置中添加服务器上HIVE_HOME下的guava-*.jar包</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;1-问题描述&quot;&gt;&lt;a href=&quot;#1-问题描述&quot; class=&quot;headerlink&quot; title=&quot;1. 问题描述&quot;&gt;&lt;/a&gt;1. 问题描述&lt;/h1&gt;&lt;p&gt;使用Dbeaver执行SELECT语句查询数据时报以下错误&lt;/p&gt;
&lt;figure class=&quot;hig</summary>
      
    
    
    
    <category term="开发工具" scheme="https://wangjiosw.github.io/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="dbeaver" scheme="https://wangjiosw.github.io/tags/dbeaver/"/>
    
    <category term="hive" scheme="https://wangjiosw.github.io/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>大数据Docker环境</title>
    <link href="https://wangjiosw.github.io/2021/03/14/bigdata/bigdata-env/"/>
    <id>https://wangjiosw.github.io/2021/03/14/bigdata/bigdata-env/</id>
    <published>2021-03-14T00:12:00.000Z</published>
    <updated>2021-05-10T14:29:52.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Spark：<a href="https://hub.docker.com/r/singularities/spark">singularities/spark</a></li><li>Kafka ：<a href="https://hub.docker.com/r/wurstmeister/kafka">wurstmeister/kafka</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;Spark：&lt;a href=&quot;https://hub.docker.com/r/singularities/spark&quot;&gt;singularities/spark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Kafka ：&lt;a href=&quot;https://hub.docker.co</summary>
      
    
    
    
    <category term="Docker" scheme="https://wangjiosw.github.io/categories/Docker/"/>
    
    
    <category term="spark" scheme="https://wangjiosw.github.io/tags/spark/"/>
    
    <category term="kafka" scheme="https://wangjiosw.github.io/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Java设计模式：原型模式</title>
    <link href="https://wangjiosw.github.io/2020/11/25/design-patterns/ProtoType/"/>
    <id>https://wangjiosw.github.io/2020/11/25/design-patterns/ProtoType/</id>
    <published>2020-11-25T06:06:00.000Z</published>
    <updated>2021-05-10T14:29:52.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>从原型实例复制克隆出新实例，而绝不是从类去实例化</p></blockquote><h1 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h1><ul><li>当希望基于现有对象创建新的对象时，比如某个类的实例很复杂，如果完全重新创建成本会很高，可以将这个实例复制一份。</li><li>参照原型进行量产（批量创建同类型对象实例）。</li></ul><h1 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h1><ul><li>向客户隐藏制造新实例的复杂性</li><li>JVM会进行内存操作直接拷贝原始数据流，简单粗暴，不会有其他更多的复杂操作（类加载，实例化，初始化等等），速度远远快于实例化操作</li></ul><h1 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h1><ul><li>对象的复制有时相当复杂</li></ul><h1 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h1><p>原型对象Phone.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Phone</span> <span class="keyword">implements</span> <span class="title class_">Cloneable</span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 手机名</span></span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="comment">// 浅拷贝只拷贝地址，需要深拷贝</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">UI</span> <span class="variable">phoneUI</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">UI</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">getName</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setName</span><span class="params">(String name)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setPhoneUI</span><span class="params">(UI phoneUI)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.phoneUI = phoneUI;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> Phone <span class="title function_">clone</span><span class="params">()</span> <span class="keyword">throws</span> CloneNotSupportedException &#123;</span><br><span class="line">        <span class="comment">// 浅拷贝</span></span><br><span class="line">        <span class="type">Phone</span> <span class="variable">clonePhone</span> <span class="operator">=</span> (Phone)<span class="built_in">super</span>.clone();</span><br><span class="line">        <span class="comment">// 深拷贝</span></span><br><span class="line">        clonePhone.setPhoneUI(<span class="built_in">this</span>.phoneUI.clone());</span><br><span class="line">        <span class="keyword">return</span> clonePhone;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对Phone进行量产的工厂</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">PhoneFactory</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 原型</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="type">Phone</span> <span class="variable">protoType</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Phone</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取克隆实例</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Phone <span class="title function_">getInstance</span><span class="params">(<span class="type">int</span> x)</span> <span class="keyword">throws</span> CloneNotSupportedException &#123;</span><br><span class="line">        <span class="comment">// 复制原型</span></span><br><span class="line">        <span class="type">Phone</span> <span class="variable">clonePhone</span> <span class="operator">=</span> protoType.clone();</span><br><span class="line">        clonePhone.setName(<span class="string">&quot;phone:&quot;</span>+x);</span><br><span class="line">        <span class="keyword">return</span> clonePhone;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>测试原型模式创建实例</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TestProtoType</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> CloneNotSupportedException &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">            <span class="type">Phone</span> <span class="variable">phone</span> <span class="operator">=</span> PhoneFactory.getInstance(i);</span><br><span class="line">            System.out.println(phone.toString()+<span class="string">&quot;\t&quot;</span>+phone.getName());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">com.wangji.prototype.Phone@61bbe9baphone:0</span><br><span class="line">com.wangji.prototype.Phone@610455d6phone:1</span><br><span class="line">com.wangji.prototype.Phone@511d50c0phone:2</span><br><span class="line">com.wangji.prototype.Phone@60e53b93phone:3</span><br><span class="line">com.wangji.prototype.Phone@5e2de80cphone:4</span><br><span class="line">com.wangji.prototype.Phone@1d44bcfaphone:5</span><br><span class="line">com.wangji.prototype.Phone@266474c2phone:6</span><br><span class="line">com.wangji.prototype.Phone@6f94fa3ephone:7</span><br><span class="line">com.wangji.prototype.Phone@5e481248phone:8</span><br><span class="line">com.wangji.prototype.Phone@66d3c617phone:9</span><br><span class="line"></span><br><span class="line">Process finished with exit code 0</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;从原型实例复制克隆出新实例，而绝不是从类去实例化&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;使用场景&quot;&gt;&lt;a href=&quot;#使用场景&quot; class=&quot;headerlink&quot; title=&quot;使用场景&quot;&gt;&lt;/a&gt;使用场景&lt;/h1&gt;&lt;ul&gt;
&lt;</summary>
      
    
    
    
    <category term="Java设计模式" scheme="https://wangjiosw.github.io/categories/Java%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
    
    
    <category term="java" scheme="https://wangjiosw.github.io/tags/java/"/>
    
    <category term="protoType" scheme="https://wangjiosw.github.io/tags/protoType/"/>
    
  </entry>
  
  <entry>
    <title>Passing Functions to Spark</title>
    <link href="https://wangjiosw.github.io/2020/11/10/bigdata/spark/Passing-Functions-to-Spark/"/>
    <id>https://wangjiosw.github.io/2020/11/10/bigdata/spark/Passing-Functions-to-Spark/</id>
    <published>2020-11-10T02:19:00.000Z</published>
    <updated>2021-05-10T14:29:52.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://spark.apache.org/docs/2.2.1/rdd-programming-guide.html">https://spark.apache.org/docs/2.2.1/rdd-programming-guide.html</a></p></blockquote><p>Spark’s API relies heavily on passing functions in the driver program to run on the cluster. There are two recommended ways to do this:</p><ul><li><a href="https://docs.scala-lang.org/tour/basics.html#functions">Anonymous function syntax</a>, which can be used for short pieces of code.</li><li>Static methods in a global singleton object. For example, you can define object MyFunctions and then pass MyFunctions.func1, as follows:</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyFunctions</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">func1</span></span>(s: <span class="type">String</span>): <span class="type">String</span> = &#123; ... &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">myRdd.map(<span class="type">MyFunctions</span>.func1)</span><br></pre></td></tr></table></figure><p>Note that while it is also possible to pass a reference to a method in a class instance (as opposed to a singleton object), this requires sending the object that contains that class along with the method. For example, consider:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">func1</span></span>(s: <span class="type">String</span>): <span class="type">String</span> = &#123; ... &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doStuff</span></span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]): <span class="type">RDD</span>[<span class="type">String</span>] = &#123; rdd.map(func1) &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Here, if we create a new MyClass instance and call doStuff on it, the map inside there references the func1 method of that MyClass instance, so the whole object needs to be sent to the cluster. It is similar to writing <strong>rdd.map(x =&gt; this.func1(x))</strong>.</p><p>In a similar way, accessing fields of the outer object will reference the whole object:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> field = <span class="string">&quot;Hello&quot;</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doStuff</span></span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]): <span class="type">RDD</span>[<span class="type">String</span>] = &#123; rdd.map(x =&gt; field + x) &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>is equivalent to writing <strong>rdd.map(x =&gt; this.field + x)</strong>, which references all of this. <strong>To avoid this issue, the simplest way is to copy field into a local variable instead of accessing it externally</strong>:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">doStuff</span></span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]): <span class="type">RDD</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> field_ = <span class="keyword">this</span>.field</span><br><span class="line">  rdd.map(x =&gt; field_ + x)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://spark.apache.org/docs/2.2.1/rdd-programming-guide.html&quot;&gt;https://spark.apache.org/docs/2.2.1/rdd-programming</summary>
      
    
    
    
    <category term="Spark" scheme="https://wangjiosw.github.io/categories/Spark/"/>
    
    
    <category term="function" scheme="https://wangjiosw.github.io/tags/function/"/>
    
  </entry>
  
  <entry>
    <title>查看kafka分区偏移量文件报错：NoSuchElementException</title>
    <link href="https://wangjiosw.github.io/2020/11/01/bigdata/kafka/kafka-dump-log-NoSuchElementException/"/>
    <id>https://wangjiosw.github.io/2020/11/01/bigdata/kafka/kafka-dump-log-NoSuchElementException/</id>
    <published>2020-11-01T02:40:00.000Z</published>
    <updated>2021-05-10T14:29:52.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="错误描述"><a href="#错误描述" class="headerlink" title="错误描述"></a>错误描述</h1><p>使用kafka-dump-log.sh 分析kafka 分区日志的.index文件时，报错</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-dump-log.sh --files /kafka/kafka-logs-5dbba1c65ae8/topic-demo-2/00000000000000000000.index </span><br><span class="line">Dumping /kafka/kafka-logs-5dbba1c65ae8/topic-demo-2/00000000000000000000.index</span><br><span class="line">Exception in thread &quot;main&quot; java.util.NoSuchElementException</span><br><span class="line">        at org.apache.kafka.common.utils.AbstractIterator.next(AbstractIterator.java:52)</span><br><span class="line">        at kafka.tools.DumpLogSegments$.$anonfun$dumpIndex$1(DumpLogSegments.scala:140)</span><br><span class="line">        at kafka.tools.DumpLogSegments$.dumpIndex(DumpLogSegments.scala:132)</span><br><span class="line">        at kafka.tools.DumpLogSegments$.$anonfun$main$1(DumpLogSegments.scala:59)</span><br><span class="line">        at kafka.tools.DumpLogSegments$.main(DumpLogSegments.scala:48)</span><br><span class="line">        at kafka.tools.DumpLogSegments.main(DumpLogSegments.scala)</span><br></pre></td></tr></table></figure><h1 id="错误原因"><a href="#错误原因" class="headerlink" title="错误原因"></a>错误原因</h1><p>日志里面没消息<br><img src="/images/bigdata/kafka/show-index.png"></p><h1 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h1><p>换一个日志里面有消息的日志对应的.index文件查看或往该分区写入消息<br><img src="/images/bigdata/kafka/show-index2.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;错误描述&quot;&gt;&lt;a href=&quot;#错误描述&quot; class=&quot;headerlink&quot; title=&quot;错误描述&quot;&gt;&lt;/a&gt;错误描述&lt;/h1&gt;&lt;p&gt;使用kafka-dump-log.sh 分析kafka 分区日志的.index文件时，报错&lt;/p&gt;
&lt;figure clas</summary>
      
    
    
    
    <category term="Kafka" scheme="https://wangjiosw.github.io/categories/Kafka/"/>
    
    
    <category term="parittion" scheme="https://wangjiosw.github.io/tags/parittion/"/>
    
  </entry>
  
  <entry>
    <title>kafka主题的分区分配策略（二）</title>
    <link href="https://wangjiosw.github.io/2020/10/26/bigdata/kafka/kafka-topic-partition2/"/>
    <id>https://wangjiosw.github.io/2020/10/26/bigdata/kafka/kafka-topic-partition2/</id>
    <published>2020-10-26T14:04:00.000Z</published>
    <updated>2021-05-10T14:29:52.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="使用该分区策略的条件"><a href="#使用该分区策略的条件" class="headerlink" title="使用该分区策略的条件"></a>使用该分区策略的条件</h1><p>broker包含机架信息，且未使用replica-assignment参数</p><h1 id="和kafka主题的分区分配策略（一）的相同和不同之处"><a href="#和kafka主题的分区分配策略（一）的相同和不同之处" class="headerlink" title="和kafka主题的分区分配策略（一）的相同和不同之处"></a>和kafka主题的分区分配策略（一）的相同和不同之处</h1><p>假设目前有3个机架，9个broker，机架和broker结点的对照关系入下表<br>机架  | broker<br>—|—<br>rack1 | 0，1，2<br>rack2 | 3，4，5<br>rack3 | 6，7，8</p><h2 id="相同点"><a href="#相同点" class="headerlink" title="相同点"></a>相同点</h2><p>分区的broker分配步骤和<a href="../kafka-topic-partition1">kafka主题的分区分配策略（一）</a>基本相同。</p><h2 id="不同点"><a href="#不同点" class="headerlink" title="不同点"></a>不同点</h2><h3 id="不同点1"><a href="#不同点1" class="headerlink" title="不同点1"></a>不同点1</h3><p><a href="../kafka-topic-partition1">kafka主题的分区分配策略（一）</a>的assignReplicasToBrokersRackUnaware()方法里的brokerArray变量的值为[0,1,2,3,4,5,6,7,8]</p><p>而assignReplicasToBrokersRackUnaware()方法里的brokerArray变量的值为[0,3,6,1,4,7,2,5,8]，这是论询各个机架产生的结果，如此新的brokerArray中包含了简单的机架分配信息。</p><h3 id="不同点2"><a href="#不同点2" class="headerlink" title="不同点2"></a>不同点2</h3><p>给分区分配的broker需要经过一层过滤</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;使用该分区策略的条件&quot;&gt;&lt;a href=&quot;#使用该分区策略的条件&quot; class=&quot;headerlink&quot; title=&quot;使用该分区策略的条件&quot;&gt;&lt;/a&gt;使用该分区策略的条件&lt;/h1&gt;&lt;p&gt;broker包含机架信息，且未使用replica-assignment参数&lt;</summary>
      
    
    
    
    <category term="Kafka" scheme="https://wangjiosw.github.io/categories/Kafka/"/>
    
    
    <category term="topic" scheme="https://wangjiosw.github.io/tags/topic/"/>
    
    <category term="partition" scheme="https://wangjiosw.github.io/tags/partition/"/>
    
  </entry>
  
  <entry>
    <title>kafka主题的分区分配策略（一）</title>
    <link href="https://wangjiosw.github.io/2020/10/26/bigdata/kafka/kafka-topic-partition1/"/>
    <id>https://wangjiosw.github.io/2020/10/26/bigdata/kafka/kafka-topic-partition1/</id>
    <published>2020-10-26T13:40:00.000Z</published>
    <updated>2021-05-10T14:29:52.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="使用该分区策略的条件"><a href="#使用该分区策略的条件" class="headerlink" title="使用该分区策略的条件"></a>使用该分区策略的条件</h1><p>未指定机架或使用disable-rack-aware参数来创建主题，且未使用replica-assignment参数</p><h1 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h1><p>核心是遍历每个分区partition，然后从brokerArray（brokerId的列表）中选取replicationFactor个brokerId分配给这个partition。</p><h1 id="kafka-admin-AdminUtilities"><a href="#kafka-admin-AdminUtilities" class="headerlink" title="kafka.admin.AdminUtilities"></a>kafka.admin.AdminUtilities</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">assignReplicasToBrokersRackUnaware</span></span>(nPartitions: <span class="type">Int</span>,</span><br><span class="line">                                                 replicationFactor: <span class="type">Int</span>,</span><br><span class="line">                                                 brokerList: <span class="type">Seq</span>[<span class="type">Int</span>],</span><br><span class="line">                                                 fixedStartIndex: <span class="type">Int</span>,</span><br><span class="line">                                                 startPartitionId: <span class="type">Int</span>): <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">Int</span>]] = &#123;</span><br><span class="line">    <span class="keyword">val</span> ret = mutable.<span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">Int</span>]]()  </span><br><span class="line">    <span class="keyword">val</span> brokerArray = brokerList.toArray    </span><br><span class="line">    <span class="keyword">val</span> startIndex = <span class="keyword">if</span> (fixedStartIndex &gt;= <span class="number">0</span>) fixedStartIndex <span class="keyword">else</span> rand.nextInt(brokerArray.length)</span><br><span class="line">    <span class="keyword">var</span> currentPartitionId = math.max(<span class="number">0</span>, startPartitionId)</span><br><span class="line">    <span class="keyword">var</span> nextReplicaShift = <span class="keyword">if</span> (fixedStartIndex &gt;= <span class="number">0</span>) fixedStartIndex <span class="keyword">else</span> rand.nextInt(brokerArray.length)</span><br><span class="line">    <span class="keyword">for</span> (_ &lt;- <span class="number">0</span> until nPartitions) &#123;</span><br><span class="line">      <span class="keyword">if</span> (currentPartitionId &gt; <span class="number">0</span> &amp;&amp; (currentPartitionId % brokerArray.length == <span class="number">0</span>))</span><br><span class="line">        nextReplicaShift += <span class="number">1</span></span><br><span class="line">      <span class="keyword">val</span> firstReplicaIndex = (currentPartitionId + startIndex) % brokerArray.length</span><br><span class="line">      <span class="keyword">val</span> replicaBuffer = mutable.<span class="type">ArrayBuffer</span>(brokerArray(firstReplicaIndex))</span><br><span class="line">      <span class="keyword">for</span> (j &lt;- <span class="number">0</span> until replicationFactor - <span class="number">1</span>)</span><br><span class="line">        replicaBuffer += brokerArray(replicaIndex(firstReplicaIndex, nextReplicaShift, j, brokerArray.length))</span><br><span class="line">      ret.put(currentPartitionId, replicaBuffer)</span><br><span class="line">      currentPartitionId += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    ret</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">replicaIndex</span></span>(firstReplicaIndex: <span class="type">Int</span>, secondReplicaShift: <span class="type">Int</span>, replicaIndex: <span class="type">Int</span>, nBrokers: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> shift = <span class="number">1</span> + (secondReplicaShift + replicaIndex) % (nBrokers - <span class="number">1</span>)</span><br><span class="line">    (firstReplicaIndex + shift) % nBrokers</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;使用该分区策略的条件&quot;&gt;&lt;a href=&quot;#使用该分区策略的条件&quot; class=&quot;headerlink&quot; title=&quot;使用该分区策略的条件&quot;&gt;&lt;/a&gt;使用该分区策略的条件&lt;/h1&gt;&lt;p&gt;未指定机架或使用disable-rack-aware参数来创建主题，且未使用</summary>
      
    
    
    
    <category term="Kafka" scheme="https://wangjiosw.github.io/categories/Kafka/"/>
    
    
    <category term="topic" scheme="https://wangjiosw.github.io/tags/topic/"/>
    
    <category term="partition" scheme="https://wangjiosw.github.io/tags/partition/"/>
    
  </entry>
  
  <entry>
    <title>kafka主题与分区管理</title>
    <link href="https://wangjiosw.github.io/2020/10/26/bigdata/kafka/topic-partition/"/>
    <id>https://wangjiosw.github.io/2020/10/26/bigdata/kafka/topic-partition/</id>
    <published>2020-10-26T13:36:00.000Z</published>
    <updated>2021-05-10T14:29:52.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="主题"><a href="#主题" class="headerlink" title="主题"></a>主题</h1><h2 id="创建主题"><a href="#创建主题" class="headerlink" title="创建主题"></a>创建主题</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper localhost:2181/kafka --create --topic topic-demo --partitions 4 --replication-factor 1</span><br></pre></td></tr></table></figure><h2 id="查看单个主题信息"><a href="#查看单个主题信息" class="headerlink" title="查看单个主题信息"></a>查看单个主题信息</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper localhost:2181/kafka --describe --topic topic-demo</span><br></pre></td></tr></table></figure><h2 id="查看当前所有可用主题"><a href="#查看当前所有可用主题" class="headerlink" title="查看当前所有可用主题"></a>查看当前所有可用主题</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper localhost:2181/kafka -list</span><br></pre></td></tr></table></figure><h2 id="修改主题"><a href="#修改主题" class="headerlink" title="修改主题"></a>修改主题</h2><p>将topic-demo分区数修改为3</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper localhost:2181/kafka --alter --topic topic-demo --partitions 3</span><br></pre></td></tr></table></figure><h2 id="删除主题"><a href="#删除主题" class="headerlink" title="删除主题"></a>删除主题</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper localhost:2181/kafka --delete --topic topic-create-api</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;主题&quot;&gt;&lt;a href=&quot;#主题&quot; class=&quot;headerlink&quot; title=&quot;主题&quot;&gt;&lt;/a&gt;主题&lt;/h1&gt;&lt;h2 id=&quot;创建主题&quot;&gt;&lt;a href=&quot;#创建主题&quot; class=&quot;headerlink&quot; title=&quot;创建主题&quot;&gt;&lt;/a&gt;创建主题&lt;/h</summary>
      
    
    
    
    <category term="Kafka" scheme="https://wangjiosw.github.io/categories/Kafka/"/>
    
    
    <category term="topic" scheme="https://wangjiosw.github.io/tags/topic/"/>
    
    <category term="partition" scheme="https://wangjiosw.github.io/tags/partition/"/>
    
  </entry>
  
  <entry>
    <title>Hive分析搜狗用户搜索日志-数据预处理(解决乱码问题)</title>
    <link href="https://wangjiosw.github.io/2020/10/22/bigdata/hive/hive-Sougou/"/>
    <id>https://wangjiosw.github.io/2020/10/22/bigdata/hive/hive-Sougou/</id>
    <published>2020-10-22T11:34:00.000Z</published>
    <updated>2021-05-10T14:29:52.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h1><ul><li>MacOS</li><li>单机hadoop集群</li><li>Hive (内嵌模式)</li></ul><h1 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h1><h2 id="1-查看数据"><a href="#1-查看数据" class="headerlink" title="1. 查看数据"></a>1. 查看数据</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">head Sougou.reduced</span><br></pre></td></tr></table></figure><p>结果如下<br><img src="/images/bigdata/hive/sougou/origin.png"></p><p>可以发现有许多乱码</p><h2 id="2-查看文件编码"><a href="#2-查看文件编码" class="headerlink" title="2. 查看文件编码"></a>2. 查看文件编码</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">file Sougou.reduced</span><br></pre></td></tr></table></figure><p>结果如下<br><img src="/images/bigdata/hive/sougou/file.png"></p><p>网上查询得知Non-ISO extended-ASCII text代表gb18030</p><h2 id="3-使用iconv转换文件编码格式"><a href="#3-使用iconv转换文件编码格式" class="headerlink" title="3. 使用iconv转换文件编码格式"></a>3. 使用iconv转换文件编码格式</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iconv -f gb18030 -t utf-8 SogouQ.reduced&gt;SogouQ.reduced.new</span><br></pre></td></tr></table></figure><p>结果如下<br><img src="/images/bigdata/hive/sougou/conv.png"></p><!-- ## 4. 将文件中的空格和制表符全部替换为英文逗号```sed -i "" "s/\t/,/g" SogouQ.reduced.newsed -i "" "s/ /,/g" SogouQ.reduced.new``` -->]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;环境&quot;&gt;&lt;a href=&quot;#环境&quot; class=&quot;headerlink&quot; title=&quot;环境&quot;&gt;&lt;/a&gt;环境&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;MacOS&lt;/li&gt;
&lt;li&gt;单机hadoop集群&lt;/li&gt;
&lt;li&gt;Hive (内嵌模式)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id</summary>
      
    
    
    
    <category term="Hive" scheme="https://wangjiosw.github.io/categories/Hive/"/>
    
    
    <category term="macos" scheme="https://wangjiosw.github.io/tags/macos/"/>
    
    <category term="sougou" scheme="https://wangjiosw.github.io/tags/sougou/"/>
    
  </entry>
  
  <entry>
    <title>Mac 单机集群使用hadoop jar 运行jar包出现的问题</title>
    <link href="https://wangjiosw.github.io/2020/10/21/bigdata/mapreduce/mac-hadoop-jar-problem/"/>
    <id>https://wangjiosw.github.io/2020/10/21/bigdata/mapreduce/mac-hadoop-jar-problem/</id>
    <published>2020-10-21T03:19:00.000Z</published>
    <updated>2021-05-10T14:29:52.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h1><p>Mac 单机集群使用hadoop jar 运行jar包报错：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.io.IOException: Mkdirs failed to create /var/folders/zz/zyxvpxvq6csfxvn_n0000000000000/T/hadoop-unjar7560830407203462827/META-INF/license</span><br><span class="line">at org.apache.hadoop.util.RunJar.ensureDirectory(RunJar.java:128)</span><br><span class="line">at org.apache.hadoop.util.RunJar.unJar(RunJar.java:104)</span><br><span class="line">at org.apache.hadoop.util.RunJar.unJar(RunJar.java:81)</span><br><span class="line">at org.apache.hadoop.util.RunJar.run(RunJar.java:209)</span><br><span class="line">at org.apache.hadoop.util.RunJar.main(RunJar.java:136)</span><br></pre></td></tr></table></figure><h1 id="2-解决方法"><a href="#2-解决方法" class="headerlink" title="2. 解决方法"></a>2. 解决方法</h1><p>Only thrown in MacOS, and solution is:<br>@see: <a href="http://stackoverflow.com/questions/10522835/hadoop-java-io-ioexception-mkdirs-failed-to-create-some-path">http://stackoverflow.com/questions/10522835/hadoop-java-io-ioexception-mkdirs-failed-to-create-some-path</a></p><ol><li>modify jar<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">zip -d mipr-core-0.1-jar-with-dependencies.jar META-INF/LICENSE</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">zip -d mipr-core-0.1-jar-with-dependencies.jar LICENSE</span></span><br></pre></td></tr></table></figure></li><li>with grep all<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">jar -tvf mipr-core-0.1-jar-with-dependencies.jar |grep META-INF/LICENSE</span></span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;1-问题描述&quot;&gt;&lt;a href=&quot;#1-问题描述&quot; class=&quot;headerlink&quot; title=&quot;1. 问题描述&quot;&gt;&lt;/a&gt;1. 问题描述&lt;/h1&gt;&lt;p&gt;Mac 单机集群使用hadoop jar 运行jar包报错：&lt;/p&gt;
&lt;figure class=&quot;hi</summary>
      
    
    
    
    <category term="Mapreduce" scheme="https://wangjiosw.github.io/categories/Mapreduce/"/>
    
    
    <category term="macos" scheme="https://wangjiosw.github.io/tags/macos/"/>
    
    <category term="hadoop jar" scheme="https://wangjiosw.github.io/tags/hadoop-jar/"/>
    
  </entry>
  
  <entry>
    <title>使用fastjson时堆溢出</title>
    <link href="https://wangjiosw.github.io/2020/10/21/jvm/fastjson-heap-oom/"/>
    <id>https://wangjiosw.github.io/2020/10/21/jvm/fastjson-heap-oom/</id>
    <published>2020-10-21T02:52:00.000Z</published>
    <updated>2021-05-10T14:29:52.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> JSONArray <span class="title function_">getChildInfo</span><span class="params">(String root, Set&lt;String&gt; childrens)</span> <span class="keyword">throws</span> JSONException &#123;</span><br><span class="line"><span class="type">JSONArray</span> <span class="variable">rtJA</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JSONArray</span>();</span><br><span class="line">    <span class="keyword">for</span> (String child : childrens) &#123;</span><br><span class="line">      <span class="type">String</span> <span class="variable">key</span> <span class="operator">=</span> generateKey(root,child);</span><br><span class="line">      <span class="keyword">if</span> (isVisited.contains(key)) &#123;</span><br><span class="line">        <span class="keyword">continue</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      </span><br><span class="line">      <span class="type">String</span> <span class="variable">simpleTableName</span> <span class="operator">=</span> child.substring(child.indexOf(<span class="string">&#x27;@&#x27;</span>)+<span class="number">1</span>);</span><br><span class="line">      effectNode.add(simpleTableName);</span><br><span class="line">      isVisited.add(key);</span><br><span class="line">      </span><br><span class="line">      <span class="type">JSONObject</span> <span class="variable">jsonObject</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JSONObject</span>();</span><br><span class="line">      <span class="keyword">if</span>(nodechildrens.containsKey(child))&#123;</span><br><span class="line">          <span class="type">JSONArray</span> <span class="variable">temp</span> <span class="operator">=</span> getChildInfo(child, nodechildrens.get(child));</span><br><span class="line">          <span class="keyword">if</span> (!temp.isEmpty()) &#123;</span><br><span class="line">              jsonObject.put(<span class="string">&quot;children&quot;</span>, temp.toJSONString());</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            jsonObject.put(<span class="string">&quot;children&quot;</span>, <span class="string">&quot;&quot;</span>);</span><br><span class="line">          &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          jsonObject.put(<span class="string">&quot;children&quot;</span>, <span class="string">&quot;&quot;</span>);</span><br><span class="line">      &#125;</span><br><span class="line">      jsonObject.put(<span class="string">&quot;name&quot;</span>,simpleTableName);</span><br><span class="line">      isVisited.remove(key);</span><br><span class="line">      rtJA.add(jsonObject.toString());</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> rtJA;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>temp.toJSONString()报错</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fastjson java.lang.OutOfMemoryError: Java heap space</span><br></pre></td></tr></table></figure><p>因为代码是里面用了递归，一开始以为是代码问题，debug了许久，没发现哪里逻辑错了。<br>想着之前学过JVM，就试着通过堆快照分析哪里错了</p><h1 id="2-堆快照分析"><a href="#2-堆快照分析" class="headerlink" title="2. 堆快照分析"></a>2. 堆快照分析</h1><p>首先，设置jvm参数(jdk 1.8)，使发生堆溢出时保存快照</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-XX:HeapDumpPath=/Users/wangji/Desktop/dumpfile.hprof -XX:+HeapDumpOnOutOfMemoryError</span><br></pre></td></tr></table></figure><p>之后，通过MAT分析快照<br><img src="/images/jvm/heap.png"></p><p>发现有个String有455MB的大小，使用fastjson时，又占了445MB，整体接近1G，这里才是发生堆溢出的原因。<br>知道原因后，递归过程中对字符串进行简化操作（去除”\“和其他多余的字符），最终解决问题</p><p>简化代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> JSONArray <span class="title function_">getChildInfo</span><span class="params">(String root, Set&lt;String&gt; childrens)</span> <span class="keyword">throws</span> JSONException &#123;</span><br><span class="line"><span class="type">JSONArray</span> <span class="variable">rtJA</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JSONArray</span>();</span><br><span class="line">    <span class="keyword">for</span> (String child : childrens) &#123;</span><br><span class="line">      <span class="type">String</span> <span class="variable">key</span> <span class="operator">=</span> generateKey(root,child);</span><br><span class="line">      <span class="keyword">if</span> (isVisited.contains(key)) &#123;</span><br><span class="line">        <span class="keyword">continue</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      </span><br><span class="line">      <span class="type">String</span> <span class="variable">simpleTableName</span> <span class="operator">=</span> child.substring(child.indexOf(<span class="string">&#x27;@&#x27;</span>)+<span class="number">1</span>);</span><br><span class="line">      effectNode.add(simpleTableName);</span><br><span class="line">      isVisited.add(key);</span><br><span class="line">      </span><br><span class="line">      <span class="type">JSONObject</span> <span class="variable">jsonObject</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JSONObject</span>();</span><br><span class="line">      <span class="keyword">if</span>(nodechildrens.containsKey(child))&#123;</span><br><span class="line">          <span class="type">JSONArray</span> <span class="variable">temp</span> <span class="operator">=</span> getChildInfo(child, nodechildrens.get(child));</span><br><span class="line">          <span class="keyword">if</span> (!temp.isEmpty()) &#123;</span><br><span class="line">              <span class="comment">// 主要是这句的字符串引起了堆内存溢出，对字符串简化</span></span><br><span class="line">              jsonObject.put(<span class="string">&quot;children&quot;</span>, modifyString(temp.toJSONString(),<span class="literal">false</span>));</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            jsonObject.put(<span class="string">&quot;children&quot;</span>, <span class="string">&quot;&quot;</span>);</span><br><span class="line">          &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          jsonObject.put(<span class="string">&quot;children&quot;</span>, <span class="string">&quot;&quot;</span>);</span><br><span class="line">      &#125;</span><br><span class="line">      jsonObject.put(<span class="string">&quot;name&quot;</span>,simpleTableName);</span><br><span class="line">      isVisited.remove(key);</span><br><span class="line">      rtJA.add(jsonObject.toString());</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> rtJA;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> String <span class="title function_">modifyString</span><span class="params">(String origin, <span class="type">boolean</span> removeSide)</span> &#123;</span><br><span class="line">    <span class="type">String</span> <span class="variable">out</span> <span class="operator">=</span> origin.replaceAll(<span class="string">&quot;\\\\&quot;</span>,<span class="string">&quot;&quot;</span>);</span><br><span class="line">    out = out.replaceAll(<span class="string">&quot;\&quot;\\&#123;&quot;</span>,<span class="string">&quot;&#123;&quot;</span>);</span><br><span class="line">    out = out.replaceAll(<span class="string">&quot;\\&#125;\&quot;&quot;</span>,<span class="string">&quot;&#125;&quot;</span>);</span><br><span class="line">    out = out.replaceAll(<span class="string">&quot;\&quot;\\[&quot;</span>,<span class="string">&quot;[&quot;</span>);</span><br><span class="line">    out = out.replaceAll(<span class="string">&quot;\\]\&quot;&quot;</span>,<span class="string">&quot;]&quot;</span>);</span><br><span class="line">    <span class="keyword">if</span> (removeSide) &#123;</span><br><span class="line">        out = out.substring(<span class="number">1</span>,out.length()-<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> out;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;1-问题描述&quot;&gt;&lt;a href=&quot;#1-问题描述&quot; class=&quot;headerlink&quot; title=&quot;1. 问题描述&quot;&gt;&lt;/a&gt;1. 问题描述&lt;/h1&gt;&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gu</summary>
      
    
    
    
    <category term="JVM" scheme="https://wangjiosw.github.io/categories/JVM/"/>
    
    
    <category term="java" scheme="https://wangjiosw.github.io/tags/java/"/>
    
    <category term="OutOfMemoryError" scheme="https://wangjiosw.github.io/tags/OutOfMemoryError/"/>
    
    <category term="heap" scheme="https://wangjiosw.github.io/tags/heap/"/>
    
  </entry>
  
  <entry>
    <title>循环神经网络pytorch实现</title>
    <link href="https://wangjiosw.github.io/2020/04/20/deep-learning/rnn-pytorch/"/>
    <id>https://wangjiosw.github.io/2020/04/20/deep-learning/rnn-pytorch/</id>
    <published>2020-04-20T05:17:00.000Z</published>
    <updated>2021-05-10T14:29:52.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>使用pytorch简单使用循环神经网络（RNN、GRU、LSTM）</p><h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><p><img src="https://img-blog.csdnimg.cn/20190717103259394.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt="RNN"><br><strong>前向过程：</strong></p><ul><li>$h_t = g(Uh_{t-1} + Wx_t +b_h)$</li><li>$y_t = g(W_yh_t + b_y)$</li></ul><h2 id="pytorch-实现"><a href="#pytorch-实现" class="headerlink" title="pytorch 实现"></a>pytorch 实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RNNCell</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(RNNCell, self).__init__()</span><br><span class="line">        self.input_size = input_size</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.linear1 = nn.Linear(hidden_dim, hidden_dim)</span><br><span class="line">        self.linear2 = nn.Linear(input_size, hidden_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, h_pre</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param x:       (batch, input_size)</span></span><br><span class="line"><span class="string">        :param h_pre:   (batch, hidden_dim)</span></span><br><span class="line"><span class="string">        :return: h_next (batch, hidden_dim)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        h_next = torch.tanh(self.linear1(h_pre) + self.linear2(x))</span><br><span class="line">        <span class="keyword">return</span> h_next</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RNN</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(RNN, self).__init__()</span><br><span class="line">        self.input_size = input_size</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.rnn_cell = RNNCell(input_size, hidden_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param x: (seq_len, batch,input_size)</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">           output (seq_len, batch, hidden_dim)</span></span><br><span class="line"><span class="string">           h_n    (1, batch, hidden_dim)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        seq_len, batch, _ = x.shape</span><br><span class="line">        h = torch.zeros(batch, self.hidden_dim)</span><br><span class="line">        output = torch.zeros(seq_len, batch, self.hidden_dim)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(seq_len):</span><br><span class="line">            inp = x[i, :, :]</span><br><span class="line">            h = self.rnn_cell(inp, h)</span><br><span class="line">            output[i, :, :] = h</span><br><span class="line"></span><br><span class="line">        h_n = output[-<span class="number">1</span>:, :, :]</span><br><span class="line">        <span class="keyword">return</span> output, h_n</span><br></pre></td></tr></table></figure><h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p><img src="https://img-blog.csdnimg.cn/20190717103242316.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt="LSTM"><br><strong>前向过程：</strong></p><ul><li>输入门: $i_t = \sigma (W_ix_t + U_ih_{t-1} + b_i)$</li><li>遗忘门: $f_t = \sigma (W_fx_t + U_fh_{t-1} + b_f)$</li><li>输出门: $o_t = \sigma (W_ox_t + U_oh_{t-1} + b_o)$</li><li>$\hat{c}<em>t = tanh(W_cx_t + U_ch</em>{t-1} + b_c)$</li><li>$c_t = f_t \odot c_{t-1} + i_t \odot \hat{c} _t$</li><li>$h_t = o_t \odot tanh(c_t)$</li></ul><h2 id="pytorch-实现-1"><a href="#pytorch-实现-1" class="headerlink" title="pytorch 实现"></a>pytorch 实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Gate</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(Gate, self).__init__()</span><br><span class="line">        self.linear1 = nn.Linear(hidden_dim, hidden_dim)</span><br><span class="line">        self.linear2 = nn.Linear(input_size, hidden_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, h_pre, active_func</span>):</span><br><span class="line">        h_next = active_func(self.linear1(h_pre) + self.linear2(x))</span><br><span class="line">        <span class="keyword">return</span> h_next</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clones</span>(<span class="params">module, N</span>):</span><br><span class="line">    <span class="string">&quot;Produce N identical layers.&quot;</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LSTMCell</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(LSTMCell, self).__init__()</span><br><span class="line">        self.input_size = input_size</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.gate = clones(Gate(input_size, hidden_dim), <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, h_pre, c_pre</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param x: (batch, input_size)</span></span><br><span class="line"><span class="string">        :param h_pre: (batch, hidden_dim)</span></span><br><span class="line"><span class="string">        :param c_pre: (batch, hidden_dim)</span></span><br><span class="line"><span class="string">        :return: h_next(batch, hidden_dim), c_next(batch, hidden_dim)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        f_t = self.gate[<span class="number">0</span>](x, h_pre, torch.sigmoid)</span><br><span class="line">        i_t = self.gate[<span class="number">1</span>](x, h_pre, torch.sigmoid)</span><br><span class="line">        g_t = self.gate[<span class="number">2</span>](x, h_pre, torch.tanh)</span><br><span class="line">        o_t = self.gate[<span class="number">3</span>](x, h_pre, torch.sigmoid)</span><br><span class="line">        c_next = f_t * c_pre + i_t * g_t</span><br><span class="line">        h_next = o_t * torch.tanh(c_next)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> h_next, c_next</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LSTM</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(LSTM, self).__init__()</span><br><span class="line">        self.input_size = input_size</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.lstm_cell = LSTMCell(input_size, hidden_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param x: (seq_len, batch,input_size)</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">           output (seq_len, batch, hidden_dim)</span></span><br><span class="line"><span class="string">           h_n    (1, batch, hidden_dim)</span></span><br><span class="line"><span class="string">           c_n    (1, batch, hidden_dim)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        seq_len, batch, _ = x.shape</span><br><span class="line">        h = torch.zeros(batch, self.hidden_dim)</span><br><span class="line">        c = torch.zeros(batch, self.hidden_dim)</span><br><span class="line">        output = torch.zeros(seq_len, batch, self.hidden_dim)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(seq_len):</span><br><span class="line">            inp = x[i, :, :]</span><br><span class="line">            h, c = self.lstm_cell(inp, h, c)</span><br><span class="line">            output[i, :, :] = h</span><br><span class="line"></span><br><span class="line">        h_n = output[-<span class="number">1</span>:, :, :]</span><br><span class="line">        <span class="keyword">return</span> output, (h_n, c.unsqueeze(<span class="number">0</span>))</span><br></pre></td></tr></table></figure><h1 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h1><p><img src="https://img-blog.csdnimg.cn/20190717103323952.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt="GRU"><br><strong>前向过程：</strong></p><p>更新门:</p><ul><li>$r_t = \sigma (W_{xr}x_t + W_{hr}h_{t-1} + b_r)$</li><li>$z_t = \sigma (W_{xz}x_t + W_{hz}h_{t-1} + b_z)$</li></ul><p>候选隐含状态：</p><ul><li>$\hat{h}<em>t = tanh(W</em>{xh}x_t + r_t \odot W_{hh}h_{t-1} + b_h)$</li></ul><p>隐含状态：</p><ul><li>$h_t = z_t \odot h_{t-1} + (1-z_t) \odot \hat{h}_t$</li></ul><p>输出:</p><ul><li>$y_t = softmax(W_{hy}h_t + b_y)$</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;使用pytorch简单使用循环神经网络（RNN、GRU、LSTM）&lt;/p&gt;
&lt;h1 id=&quot;RNN&quot;&gt;&lt;a href=&quot;#RNN&quot; clas</summary>
      
    
    
    
    <category term="深度学习" scheme="https://wangjiosw.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="pytorch" scheme="https://wangjiosw.github.io/tags/pytorch/"/>
    
    <category term="RNN" scheme="https://wangjiosw.github.io/tags/RNN/"/>
    
    <category term="GRU" scheme="https://wangjiosw.github.io/tags/GRU/"/>
    
    <category term="LSTM" scheme="https://wangjiosw.github.io/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>矩阵求导方法</title>
    <link href="https://wangjiosw.github.io/2020/04/18/deep-learning/matrix-partial/"/>
    <id>https://wangjiosw.github.io/2020/04/18/deep-learning/matrix-partial/</id>
    <published>2020-04-18T05:17:00.000Z</published>
    <updated>2022-05-08T01:16:40.845Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、全微分"><a href="#1、全微分" class="headerlink" title="1、全微分"></a>1、全微分</h1><ul><li><p>当X是矩阵，$dy = tr(\frac{\partial y}{\partial X}^T dX)$</p></li><li><p>当X是向量，$dy = \frac{\partial y}{\partial X}^T dX=tr(\frac{\partial y}{\partial X}^T dX)$</p></li></ul><h1 id="2、活用迹（tr）"><a href="#2、活用迹（tr）" class="headerlink" title="2、活用迹（tr）"></a>2、活用迹（tr）</h1><p>$(1) a是标量，a = tr(a)$</p><p>$(2) A，B为方阵，tr(AB) = tr(BA)$</p><p>$(3) tr(A) = tr(A^T)$</p><p>$(4) tr(A+B) = tr(A)+tr(B)$</p><p>$(5) 微分d(X^T) = (dX)^T$</p><p>这些公式将用于下面的求导</p><h1 id="3、矩阵求导例子"><a href="#3、矩阵求导例子" class="headerlink" title="3、矩阵求导例子"></a>3、矩阵求导例子</h1><p>下面将展示一些用上面公式求矩阵导数的例子：</p><hr><h2 id="例1："><a href="#例1：" class="headerlink" title="例1："></a>例1：</h2><p>$X = (x_1,…,x_n)^T$是向量，$A$是与$X$无关的矩阵：$y = X^TAX ，求 \frac{\partial y}{\partial X}?$</p><p>全微分表达式：<br>$dy = (dX^T)AX + X^TA(dX)$</p><p>由公式（1）得：</p><p>$dy = tr((dX^T)AX + X^TA(dX))$</p><p>由公式（2）得：</p><p>$dy = tr((dX^T)AX)+tr( X^TA(dX))$</p><p>由公式(5) (3)得：</p><p>$dy =  tr((dX)^TAX)+tr( X^TA(dX))= tr(X^TA^TdX)+tr( X^TA(dX))$</p><p>由公式（2）得：</p><p>$dy = tr(X^TA^T(dX) + X^TA(dX))=tr(X^T(A^T+A)dX)$</p><p>$\because dy = tr(\frac{\partial y}{\partial X}^T dX)$</p><p>$\Rightarrow \frac{\partial y}{\partial X}^T = X^T(A^T+A)$</p><p>$\Rightarrow \frac{\partial y}{\partial X} = (X^T(A^T+A))^T= (A+A^T)X$</p><hr><h2 id="例2："><a href="#例2：" class="headerlink" title="例2："></a>例2：</h2><p>$y = tr(AB),求\frac{\partial y}{\partial A}?$</p><p>全微分表达式：<br>$dy = tr[(dA)B]$</p><p>由公式(2)得：<br>$dy = tr[B\ dA]$</p><p>$\because dy = tr(\frac{\partial y}{\partial X}^T dX)$</p><p>$\Rightarrow \frac{\partial y}{\partial A}^T = B$</p><p>$\Rightarrow \frac{\partial y}{\partial A} =B^T$</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;1、全微分&quot;&gt;&lt;a href=&quot;#1、全微分&quot; class=&quot;headerlink&quot; title=&quot;1、全微分&quot;&gt;&lt;/a&gt;1、全微分&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;当X是矩阵，$dy = tr(\frac{\partial y}{\partial X}^T dX</summary>
      
    
    
    
    <category term="深度学习" scheme="https://wangjiosw.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="矩阵求导" scheme="https://wangjiosw.github.io/tags/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/"/>
    
  </entry>
  
  <entry>
    <title>Hbase创建表出错</title>
    <link href="https://wangjiosw.github.io/2020/03/30/bigdata/hbase/hbase-create-table-error/"/>
    <id>https://wangjiosw.github.io/2020/03/30/bigdata/hbase/hbase-create-table-error/</id>
    <published>2020-03-30T09:14:00.000Z</published>
    <updated>2021-05-10T14:29:52.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):002:0&gt; create &#x27;test&#x27;,&#x27;cf&#x27;</span><br><span class="line"></span><br><span class="line">ERROR: java.io.IOException: Table Namespace Manager not ready yet, try again later</span><br><span class="line">    at org.apache.hadoop.hbase.master.HMaster.getNamespaceDescriptor(HMaster.java:3172)</span><br><span class="line">    at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:1727)</span><br><span class="line">    at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:1766)</span><br></pre></td></tr></table></figure><h1 id="2-错误的原因"><a href="#2-错误的原因" class="headerlink" title="2. 错误的原因"></a>2. 错误的原因</h1><p>Hbase底层hadoop集群时间不一致</p><h1 id="3-解决方法"><a href="#3-解决方法" class="headerlink" title="3. 解决方法"></a>3. 解决方法</h1><h2 id="3-1-安装ntpdate工具"><a href="#3-1-安装ntpdate工具" class="headerlink" title="3.1 安装ntpdate工具"></a>3.1 安装ntpdate工具</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install ntp ntpdate</span><br></pre></td></tr></table></figure><h2 id="3-2-设置系统时间与网络时间同步"><a href="#3-2-设置系统时间与网络时间同步" class="headerlink" title="3.2 设置系统时间与网络时间同步"></a>3.2 设置系统时间与网络时间同步</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ntpdate cn.pool.ntp.org</span><br></pre></td></tr></table></figure><h1 id="4-常用NTP服务器地址"><a href="#4-常用NTP服务器地址" class="headerlink" title="4. 常用NTP服务器地址"></a>4. 常用NTP服务器地址</h1><h2 id="4-1-阿里云授时服务器"><a href="#4-1-阿里云授时服务器" class="headerlink" title="4.1 阿里云授时服务器"></a>4.1 阿里云授时服务器</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#NTP服务器</span><br><span class="line">ntp.aliyun.com             </span><br><span class="line">ntp1.aliyun.com</span><br><span class="line">ntp2.aliyun.com</span><br><span class="line">ntp3.aliyun.com</span><br><span class="line">ntp4.aliyun.com</span><br><span class="line">ntp5.aliyun.com</span><br><span class="line">ntp6.aliyun.com</span><br><span class="line">ntp7.aliyun.com</span><br><span class="line"></span><br><span class="line">#Time服务器</span><br><span class="line">time1.aliyun.com</span><br><span class="line">time2.aliyun.com</span><br><span class="line">time3.aliyun.com</span><br><span class="line">time4.aliyun.com</span><br><span class="line">time5.aliyun.com</span><br><span class="line">time6.aliyun.com</span><br><span class="line">time7.aliyun.com</span><br></pre></td></tr></table></figure><h2 id="4-2-国内大学授时服务器"><a href="#4-2-国内大学授时服务器" class="headerlink" title="4.2 国内大学授时服务器"></a>4.2 国内大学授时服务器</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">s1c.time.edu.cn       北京大学 </span><br><span class="line">s2m.time.edu.cn       北京大学</span><br><span class="line">s1b.time.edu.cn       清华大学</span><br><span class="line">s1e.time.edu.cn       清华大学</span><br><span class="line">s2a.time.edu.cn       清华大学</span><br><span class="line">s2b.time.edu.cn       清华大学</span><br></pre></td></tr></table></figure><h2 id="4-3-国外授时服务器"><a href="#4-3-国外授时服务器" class="headerlink" title="4.3 国外授时服务器"></a>4.3 国外授时服务器</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#苹果提供的授时服务器   </span><br><span class="line">time1.apple.com</span><br><span class="line">time2.apple.com</span><br><span class="line">time3.apple.com</span><br><span class="line">time4.apple.com</span><br><span class="line">time5.apple.com</span><br><span class="line">time6.apple.com</span><br><span class="line">time7.apple.com</span><br><span class="line"></span><br><span class="line">#Google提供的授时服务器   </span><br><span class="line">time1.google.com</span><br><span class="line">time2.google.com</span><br><span class="line">time3.google.com</span><br><span class="line">time4.google.com</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;1-问题描述&quot;&gt;&lt;a href=&quot;#1-问题描述&quot; class=&quot;headerlink&quot; title=&quot;1. 问题描述&quot;&gt;&lt;/a&gt;1. 问题描述&lt;/h1&gt;&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td clas</summary>
      
    
    
    
    <category term="Hbase" scheme="https://wangjiosw.github.io/categories/Hbase/"/>
    
    
    <category term="ntpdate" scheme="https://wangjiosw.github.io/tags/ntpdate/"/>
    
  </entry>
  
  <entry>
    <title>安装hive时jline版本不一致问题</title>
    <link href="https://wangjiosw.github.io/2020/03/17/bigdata/hive/hive-install-jline-error/"/>
    <id>https://wangjiosw.github.io/2020/03/17/bigdata/hive/hive-install-jline-error/</id>
    <published>2020-03-17T07:14:00.000Z</published>
    <updated>2021-05-10T14:29:52.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">将hive包解压后，运行bin目录下的hive脚本，报以下错误：</span><br><span class="line">[ERROR] Terminal initialization failed; falling back to unsupported</span><br><span class="line">java.lang.IncompatibleClassChangeError: Found class jline.Terminal, but interface was expected</span><br><span class="line">at jline.TerminalFactory.create(TerminalFactory.java:101)</span><br></pre></td></tr></table></figure><h1 id="2-错误的原因"><a href="#2-错误的原因" class="headerlink" title="2. 错误的原因"></a>2. 错误的原因</h1><p>Hadoop jline版本和hive的jline不一致</p><h1 id="3-解决方法"><a href="#3-解决方法" class="headerlink" title="3. 解决方法"></a>3. 解决方法</h1><p>删除your_install_path/hadoop/share/hadoop/yarn/lib目录下的jline包,然后把hive里面的jline包拷过来。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;1-问题描述&quot;&gt;&lt;a href=&quot;#1-问题描述&quot; class=&quot;headerlink&quot; title=&quot;1. 问题描述&quot;&gt;&lt;/a&gt;1. 问题描述&lt;/h1&gt;&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td clas</summary>
      
    
    
    
    <category term="Hive" scheme="https://wangjiosw.github.io/categories/Hive/"/>
    
    
    <category term="jline" scheme="https://wangjiosw.github.io/tags/jline/"/>
    
  </entry>
  
  <entry>
    <title>本地运行mapred问题</title>
    <link href="https://wangjiosw.github.io/2020/03/17/bigdata/mapreduce/mapred-localmode-error/"/>
    <id>https://wangjiosw.github.io/2020/03/17/bigdata/mapreduce/mapred-localmode-error/</id>
    <published>2020-03-17T06:58:00.000Z</published>
    <updated>2021-05-10T14:29:52.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h1><p>在hadoop集群上跑mapreduce代码很慢，可以通过设置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conf.set(&quot;mapreduce.framework.name&quot;, &quot;local&quot;);</span><br></pre></td></tr></table></figure><p>使mapreduce代码能在本地运行。</p><p>运行时报以下错误：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2020</span>-<span class="number">03</span>-<span class="number">14</span> <span class="number">20</span>:<span class="number">24</span>:<span class="number">30</span>,<span class="number">241</span> WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:&lt;clinit&gt;(<span class="number">62</span>)) - Unable to load <span class="keyword">native</span>-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes where applicable</span><br><span class="line"><span class="number">2020</span>-<span class="number">03</span>-<span class="number">14</span> <span class="number">20</span>:<span class="number">24</span>:<span class="number">31</span>,<span class="number">625</span> INFO  [main] Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(<span class="number">1129</span>)) - session.id is deprecated. Instead, use dfs.metrics.session-id</span><br><span class="line"><span class="number">2020</span>-<span class="number">03</span>-<span class="number">14</span> <span class="number">20</span>:<span class="number">24</span>:<span class="number">31</span>,<span class="number">626</span> INFO  [main] jvm.JvmMetrics (JvmMetrics.java:init(<span class="number">76</span>)) - Initializing JVM Metrics with processName=JobTracker, sessionId=</span><br><span class="line"><span class="number">2020</span>-<span class="number">03</span>-<span class="number">14</span> <span class="number">20</span>:<span class="number">24</span>:<span class="number">31</span>,<span class="number">924</span> WARN  [main] mapreduce.JobResourceUploader (JobResourceUploader.java:uploadFiles(<span class="number">64</span>)) - Hadoop command-line option parsing not performed. Implement the Tool <span class="keyword">interface</span> <span class="title class_">and</span> execute your application with ToolRunner to remedy <span class="built_in">this</span>.</span><br><span class="line"><span class="number">2020</span>-<span class="number">03</span>-<span class="number">14</span> <span class="number">20</span>:<span class="number">24</span>:<span class="number">31</span>,<span class="number">948</span> INFO  [main] mapreduce.JobSubmitter (JobSubmitter.java:submitJobInternal(<span class="number">250</span>)) - Cleaning up the staging area file:/<span class="keyword">var</span>/sxt/hadoop/ha/mapred/staging/wangji241520974/.staging/job_local241520974_0001</span><br><span class="line">ExitCodeException exitCode=<span class="number">1</span>: chmod: /<span class="keyword">private</span>/<span class="keyword">var</span>/sxt/hadoop/ha/mapred/staging/wangji241520974/.staging/job_local241520974_0001: No such file or directory</span><br></pre></td></tr></table></figure><h1 id="2-解决方法"><a href="#2-解决方法" class="headerlink" title="2. 解决方法"></a>2. 解决方法</h1><p>在/private/var目录下创建sxt目录,并把权限改为777</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="built_in">mkdir</span> sxt</span><br><span class="line">sudo <span class="built_in">chmod</span> 777 sxt</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;1-问题描述&quot;&gt;&lt;a href=&quot;#1-问题描述&quot; class=&quot;headerlink&quot; title=&quot;1. 问题描述&quot;&gt;&lt;/a&gt;1. 问题描述&lt;/h1&gt;&lt;p&gt;在hadoop集群上跑mapreduce代码很慢，可以通过设置&lt;/p&gt;
&lt;figure class=&quot;h</summary>
      
    
    
    
    <category term="Mapreduce" scheme="https://wangjiosw.github.io/categories/Mapreduce/"/>
    
    
    <category term="macos" scheme="https://wangjiosw.github.io/tags/macos/"/>
    
    <category term="hadoop" scheme="https://wangjiosw.github.io/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>胶囊间的动态路由-论文解读</title>
    <link href="https://wangjiosw.github.io/2020/03/01/deep-learning/capsule/"/>
    <id>https://wangjiosw.github.io/2020/03/01/deep-learning/capsule/</id>
    <published>2020-03-01T05:25:00.000Z</published>
    <updated>2021-05-10T14:29:52.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Capsule介绍"><a href="#1-Capsule介绍" class="headerlink" title="1. Capsule介绍"></a>1. Capsule介绍</h1><blockquote><p>Sabour, Sara, Nicholas Frosst, and Geoffrey E. Hinton. “Dynamic routing between capsules.” Advances in neural information processing systems. 2017.</p></blockquote><p>Capsule特色是“vector in vector out”，取代了以往的“scaler in scaler out”，也就是神经元的输入输出都变成了向量，从而算是对神经网络理论的一次革命。</p><p>然而在目前的深度学习中，从来不缺乏“vector in vector out”的案例，因此显然这不能算是Capsule的革命。比如在NLP中，一个词向量序列的输入模型，这个词向量序列再经过RNN/CNN/Attention的编码，输出一个新序列，不也是“vector in vector out”吗？</p><p>Capsule的革命在于：<strong>它提出了一种新的“vector in vector out”的传递方案，并且这种方案在很大程度上是可解释的。</strong></p><p>深度学习（神经网络）为什么有效：神经网络通过层层叠加完成了对输入的层层抽象，这个过程某种程度上<strong>模拟了人的层次分类做法</strong>，从而完成对最终目标的输出，并且具有比较好的泛化能力。的确，神经网络应该是这样做的，然而它并不能告诉我们它确确实实是这样做的，这就是神经网络的难解释性，也就是很多人会将深度学习视为黑箱的原因之一。</p><p>下面介绍Capsule是怎么突破这一点的。</p><h1 id="2-CapsNet模型"><a href="#2-CapsNet模型" class="headerlink" title="2. CapsNet模型"></a>2. CapsNet模型</h1><p>CapsNet: 两个卷积层(Conv 1, PrimaryCaps)，一个全连接层(DigitCaps)</p><p><img src="https://img-blog.csdnimg.cn/20190615192127397.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="2-1-Conv1层"><a href="#2-1-Conv1层" class="headerlink" title="2.1 Conv1层"></a>2.1 Conv1层</h2><p>常规的卷积层,  起像素级局部特征检测作用</p><p><img src="https://img-blog.csdnimg.cn/20190615192144318.png" alt="在这里插入图片描述"><br>$shape: [None,28,28,1] \rightarrow [None,20,20,256]$</p><h2 id="2-2-PrimaryCaps层"><a href="#2-2-PrimaryCaps层" class="headerlink" title="2.2 PrimaryCaps层"></a>2.2 PrimaryCaps层</h2><p>生成最低级卷积8D<strong>胶囊</strong>层（无路由）</p><p>胶囊：其实，<strong>只要把一个向量当作一个整体来看，它就是一个“胶囊”。可以这样理解：神经元就是标量，胶囊就是向量</strong>。Hinton的理解是：每一个胶囊表示一个属性，而胶囊的向量则表示这个属性的“标架”。也就是说，我们以前只是用一个标量表示有没有这个特征（比如有没有羽毛），现在我们用一个向量来表示，不仅仅表示有没有，还表示“有什么样的”（比如有什么颜色、什么纹理的羽毛），如果这样理解，就是说在对单个特征的表达上更丰富了。</p><p>简单的讲，PrimaryCaps层要输出一些8D的向量，每个向量代表一些比较低级的特征，向量的各个位置的值代表该特征的属性</p><p><strong>PrimaryCaps层: 计算过程具有多种理解方式，其中之一为，8个并行的常规卷积层的叠堆</strong></p><h3 id="PrimaryCaps层的第一种理解方式"><a href="#PrimaryCaps层的第一种理解方式" class="headerlink" title="PrimaryCaps层的第一种理解方式"></a>PrimaryCaps层的第一种理解方式</h3><p>8个并行的常规卷积层:<br><img src="https://img-blog.csdnimg.cn/20190615192158370.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>卷积操作2参数:<br><img src="https://img-blog.csdnimg.cn/20190615192208901.png" alt="在这里插入图片描述"><br>对Conv1层的输出进行8次卷积操作2所示的卷积操作：<br><img src="https://img-blog.csdnimg.cn/20190615192232750.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>然后对8个并行常规卷积层叠堆（对每个卷积层的各个通道在第四个维度上进行合并）：</p><p>8个[6,6,1,32]卷积层合并示意图如下:<br><img src="https://img-blog.csdnimg.cn/20190615192257910.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>得到叠堆后的结果：[None,6,6,8,32]<br>然后展开为[None,6x6x32,8,1]=[None,1152,8,1]，这样我们就得到了1152个初始胶囊，每个胶囊是一个纬度为[8,1]的向量，并代表某一特征</p><h3 id="PrimaryCaps层的第二种理解方式"><a href="#PrimaryCaps层的第二种理解方式" class="headerlink" title="PrimaryCaps层的第二种理解方式"></a>PrimaryCaps层的第二种理解方式</h3><ul><li>32个通道之间的卷积核是独立的(9x9大小)</li><li>8个并行卷积层之间的参数也是独立的</li></ul><p>$Rightarrow$ 即共有8x32个大小为9x9的相互独立的卷积核，可看作8x32个通道的常规卷积和<br>则可以用下面的操作得到和第一种理解方式相同的结果<br><img src="https://img-blog.csdnimg.cn/2019061519232534.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>输出：$[None,6,6,32*8] \overset{reshape}{\rightarrow} [None,1152,8,1])$</p><p><strong>注意</strong>：虽然计算方式上与常规卷积层无差异，但意义上却已经大不相同！将达到8x1capsule的特征封装的效果</p><p><strong>PrimaryCaps输出输出1152个8D的胶囊后，使用了一个Squash函数做非线性变换，那么，为什么要设计这个函数呢？这个函数的原理是啥</strong></p><h2 id="2-3-Squash函数"><a href="#2-3-Squash函数" class="headerlink" title="2.3 Squash函数"></a>2.3 Squash函数</h2><h3 id="为什么要设计squash函数："><a href="#为什么要设计squash函数：" class="headerlink" title="为什么要设计squash函数："></a>为什么要设计squash函数：</h3><p>因为论文希望Capsule能有一个性质：<strong>胶囊的模长能够代表这个特征的概率，即特征的“显著程度”， 模长越大，这个特征越显著，而我们又希望有一个有界的指标来对这个“显著程度”进行衡量，所以就只能对这个模长进行压缩了</strong></p><h3 id="squash函数的原理："><a href="#squash函数的原理：" class="headerlink" title="squash函数的原理："></a>squash函数的原理：</h3><p>$$squash(x) = \frac{||x||^2}{1+||x||^2} \frac{x}{||x||}$$</p><ul><li>$\frac{x}{||x||}$: 将x的模长变为1</li><li>$\frac{||x||^2}{1+||x||^2}$:     起缩放x的模长的作用，x模长越大，$\frac{||x||^2}{1+||x||^2}$越趋近于1，||x||=0时，$\frac{||x||^2}{1+||x||^2}=0$</li></ul><p>则$y=squash(x)$的效果为：x的模长越大，y的模长越趋近于1</p><p><strong>PrimaryCaps输出的1152个8D的胶囊经过squash函数后非线性变换后，都具有了胶囊的模长能够代表这个特征的概率的特性，这些新的胶囊接着作为DigitCaps层的输入。</strong></p><h2 id="2-4-DigitCaps层"><a href="#2-4-DigitCaps层" class="headerlink" title="2.4 DigitCaps层"></a>2.4 DigitCaps层</h2><p>由于该层解释起来比较复杂，所以先从简单例子开始，慢慢推出该层的流程及原理。</p><h3 id="capsule示意"><a href="#capsule示意" class="headerlink" title="capsule示意"></a>capsule示意</h3><p>capsule示意图:</p><p><img src="https://img-blog.csdnimg.cn/20190615192340940.png" alt="在这里插入图片描述"><br>如上图所示，底层的胶囊和高层的胶囊构成一些连接关系那么，这些胶囊要怎么运算，才能体现出“层层抽象”、“层层分类”的特性呢？让我们先看其中一部分连接：</p><p><img src="https://img-blog.csdnimg.cn/20190615192354980.png" alt="在这里插入图片描述"><br>u,v都是胶囊，图上只展示了$u_1$的连接。这也就是说，目前已经有了$u_1$这个特征（假设是羽毛），那么我想知道它属于上层特征$v_1,v_2  ,v_3,v_4$（假设分别代表了鸡、鸭、鱼、狗）中的哪一个。分类问题我们显然已经是很熟悉了，不就是内积后softmax吗？于是单靠$u_1$这个特征，我们推导出它是属于鸡、鸭、鱼、狗的概率分别是:</p><p>$$(p_{1|1},p_{2|1},p_{3|1},p_{4|1})=\frac{1}{Z_1} (e^{&lt;u_1,v_1&gt;},e^{&lt;u_1,v_2&gt;},e^{&lt;u_1,v_3&gt;},e^{&lt;u_1,v_4&gt;})$$</p><p>我们期望$p_{1|1}，p_{2|1}$会明显大于$p_{3|1}，p_{4|1}$（鸡鸭有羽毛，鱼狗没羽毛）不过，单靠这个特征还不够，我们还需要综合各个特征，于是可以把上述操作对各个u_i都做一遍，继而得到<br>$$(p_{1|2},p_{2|2},p_{3|2},p_{4|2}), (p_{1|3},p_{2|3},p_{3|3},p_{4|3}), …$$</p><p>问题是，现在得到这么多预测结果，究竟要选择哪个呢？而且又不是真的要做分类，我们要的是融合这些特征，构成更高级的特征。</p><p>于是Hinton认为，既然$u_i$这个特征得到的概率分布是$(p_{1|i},p_{2|i},p_{3|i},p_{4|i})$那么我把这个特征切成四份，分别为$(p_{1|i}u_i,p_{2|i}u_i,p_{3|i}u_i,p_{4|i}u_i)$, 然后把这几个特征分别传给$v_1,v_2,v_3,v_4$，最后$v_1,v_2,v_3,v_4$其实就是底层传入的特征的累加</p><p>$$v_j=squash(p_{j|i} u_i )=squash(\sum_{i} \frac{e^{&lt;u_i,v_j&gt;}}{Z_i} u_i)$$</p><p>从上往下看，那么Capsule就是每个底层特征分别做分类，然后将分类结果整合。这时$v_j$应该尽量与所有$u_i$都比较靠近，靠近的度量是内积。因此，从下往上看的话，可以认为$v_j$实际上就是各个$u_i$的某个聚类中心，而Capsule的核心思想就是<strong>输出是输入的某种聚类结果。</strong></p><h3 id="动态路由："><a href="#动态路由：" class="headerlink" title="动态路由："></a>动态路由：</h3><p>注意到式子$v_j=squash(\sum_{i} \frac{e^{&lt;u_i,v_j&gt;}}{Z_i} u_i)$，为了求$v_j$需要求softmax，可是为了求softmax又需要知道$v_j$，这不是个鸡生蛋、蛋生鸡的问题了吗？而“动态路由”正是为了解决这一问题而提出的，它能够根据自身的特性来更新（部分）参数，从而初步达到了Hinton的放弃梯度下降的目标</p><p>下面通过几个例子来解释动态路由的过程：</p><h3 id="例1"><a href="#例1" class="headerlink" title="例1:"></a>例1:</h3><p>让我们先回到普通的神经网络，大家知道，激活函数在神经网络中的地位是举足轻重的。当然，激活函数本身很简单，比如一个tanh激活的全连接层。</p><p>可是，如果想用$x=y+cos⁡y$的反函数来激活呢？也就是说，得解出$y=f(x)$，然后再用它来做激活函数。<br>然而$x=y+cos⁡y$的反函数是一个超越函数，也就是不可能用初等函数有限地表示出来。<br>但我们可以通过迭代法求出y：<br>$$y_{n+1} = x - cos y_n$$</p><p>选择$y_0=x$，代入上式迭代几次，基本上就可以得到比较准确的y了。假如迭代3次，那就是<br>$$y=x-cos⁡(x-cos⁡(x-cos⁡x))$$<br>可以发现这和动态路由的过程有点像</p><h3 id="例2"><a href="#例2" class="headerlink" title="例2:"></a>例2:</h3><p>再来看一个例子，这个例子可能在NLP中有很多对应的情景，但图像领域其实也不少。考虑一个向量序列$(x_1,x_2,…,x_n)$，我现在要想办法将这n个向量整合成一个向量x（encoder），然后用这个向量来做分类：<br>$$x= \sum_{i=1}^n \lambda _i x_i$$<br>这里的λ_i相当于衡量了x与x_i的相似度。那么，在x出现之前，凭什么能够确定这个相似度呢？<br>解决这个问题的一个方案也是迭代。首先我们也可以定义一个基于softmax的相似度指标，然后让</p><p>$$x= \sum_{i=1}^n \frac{e^{&lt;x,x_i&gt;}}{Z} x_i$$</p><p>一开始，我们一无所知，所以只好取x为各个$x_i$的均值，然后代入右边就可以算出一个x，再把它代入右边，反复迭代就行，一般迭代有限次就可以收敛，于是就可以将这个迭代过程嵌入到神经网络中了。</p><p>如果说例1跟动态路由只是神似，那么例2已经跟动态路由是神似＋形似了。</p><p>通过例1，例2，已经可以很清晰的开始解释动态路由过程了<br>为了得到各个v_j，一开始先让它们全都等于u_i的均值，然后反复迭代就好。说白了，输出是输入的聚类结果，而聚类通常都需要迭代算法，这个迭代算法就称为“动态路由”。</p><p>到此，就可以写出论文里的动态路由的算法了：</p><hr><p>动态路由算法<br>初始化$b_{ij}$=0<br>迭代r次：<br>&nbsp;&nbsp;&nbsp;&nbsp;$c_i \leftarrow softmax(b_i)$<br>&nbsp;&nbsp;&nbsp;&nbsp;$s_j \leftarrow \sum_i c_{ij} u_i$<br>&nbsp;&nbsp;&nbsp;&nbsp;$v_j \leftarrow squash(s_j)$<br>&nbsp;&nbsp;&nbsp;&nbsp;$b_{ij} \leftarrow b_{ij}  + &lt;u_i,v_j&gt;$<br>返回$v_j$</p><hr><p>这里的$c_{ij}$就是前文的$p_{j|i}$前面已经说了，$v_j$是作为输入$u_i$的某种聚类中心出现的，而从不同角度看输入，得到的聚类结果显然是不一样的。那么为了实现“多角度看特征”，于是可以在每个胶囊传入下一个胶囊之前，都要先乘上一个矩阵做变换，所以式$v_j=squash(\sum_{i} \frac{e^{&lt;u_i,v_j&gt;}}{Z_i} u_i)$实际上应该要变为</p><p>$v_j=squash(\sum_{i} \frac{e^{&lt;u_i,v_j&gt;}}{Z_i} \hat{u} _{j|i})$</p><p>$\hat{u}<em>{j|i} = W</em>{ji} u_i$</p><p>这里的$W_{ji}$是待训练的矩阵，这里的乘法是矩阵乘法，也就是矩阵乘以向量。所以，Capsule变成了下图<br><img src="https://img-blog.csdnimg.cn/20190615192705864.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这时候就可以得到完整动态路由了:</p><hr><p>动态路由算法<br>初始化$b_{ij}$=0<br>迭代r次：<br>&nbsp;&nbsp;&nbsp;&nbsp;$c_i \leftarrow softmax(b_i)$<br>&nbsp;&nbsp;&nbsp;&nbsp;$s_j \leftarrow \sum_i c_{ij} \hat{u}<em>{j|i}$<br>&nbsp;&nbsp;&nbsp;&nbsp;$v_j \leftarrow squash(s_j)$<br>&nbsp;&nbsp;&nbsp;&nbsp;$b</em>{ij} \leftarrow b_{ij}  + &lt;u_i,v_j&gt;$<br>返回$v_j$</p><hr><p>这样的Capsule层，显然相当于普通神经网络中的全连接层。 </p><h3 id="DigitCaps层流程总结"><a href="#DigitCaps层流程总结" class="headerlink" title="DigitCaps层流程总结"></a>DigitCaps层流程总结</h3><ol><li>将PrimaryCaps输入的1152个8D的胶囊从乘$W_{ji}$，以达到不同角度看输入的目的，得到[None,10,1152,16,1]</li><li>对每个1152里面的16D的胶囊，通过动态路由算法聚类出一个$v_j$</li><li>返回[None,10,16],即输出10个胶囊，分别对应数字0～9，胶囊的模长代表是该数字的概率，每个胶囊内部的值代表了该数字的某一属性</li></ol><h2 id="2-5-重构层"><a href="#2-5-重构层" class="headerlink" title="2.5 重构层"></a>2.5 重构层</h2><p>重构层就比较简单了，但是也有一些细节需要说明一下<br><img src="https://img-blog.csdnimg.cn/20190615192716874.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>网络很简单，就是三个全连接层，其中有个masked操作，具体原理如下：<br>因为DigitCaps输出[None,10,16]，即每个样本输出10个16D的胶囊，胶囊的模长代表图片是这个类别的概率，而每个16D的胶囊里面各个位置的值则代表了这个数字的一系列属性，重构是该胶囊已经包含了大部分的信息，假设要重构的是数字5，那么就把DigitCaps该位置的mask设置为1，其他位置为0，那么实际重构事，就只有这个胶囊的信息参与了运算。</p><h2 id="2-6-损失函数Margin-loss-recon-loss"><a href="#2-6-损失函数Margin-loss-recon-loss" class="headerlink" title="2.6 损失函数Margin loss + recon loss"></a>2.6 损失函数Margin loss + recon loss</h2><h3 id="Margin-loss函数："><a href="#Margin-loss函数：" class="headerlink" title="Margin loss函数："></a>Margin loss函数：</h3><p>$$L_c = T_c max⁡(0,m^+ - ||v_c ||)^2+ \lambda (1-T_c )max⁡(0,||v_c||-m^- )^2$$</p><ul><li>$c$:类别</li><li>$T_c$:指示函数（分类c存在为1，否则为0）</li><li>$m^-$:$||v_c ||$上边界，避免假阴性，遗漏实际预测到存在的分类的情况</li><li>$m^+$:$||v_c |)|$下边界，避免假阳性</li><li>margin loss: $\sum_c L_c$ </li></ul><h3 id="重构误差："><a href="#重构误差：" class="headerlink" title="重构误差："></a>重构误差：</h3><ul><li>作用： 正则化</li><li>重构网络： MLP</li><li>重构误差计算方式MSE</li></ul><h1 id="3-运行结果"><a href="#3-运行结果" class="headerlink" title="3. 运行结果"></a>3. 运行结果</h1><h2 id="3-1测试集分类结果"><a href="#3-1测试集分类结果" class="headerlink" title="3.1测试集分类结果"></a>3.1测试集分类结果</h2><video src="/images/capsule/capsule.mp4" controls="controls" width="640" height="320" autoplay="autoplay">Your browser does not support the video tag.</video><h3 id="论文分类结果"><a href="#论文分类结果" class="headerlink" title="论文分类结果:"></a>论文分类结果:</h3><p><img src="https://img-blog.csdnimg.cn/201906151927271.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="复现"><a href="#复现" class="headerlink" title="复现"></a>复现</h3><p>（routing:3,Reconstruction:yes）结果在测试集的准确率平均可达99.24%以上，基本复现成功</p><h2 id="3-2-单数字重构效果"><a href="#3-2-单数字重构效果" class="headerlink" title="3.2 单数字重构效果"></a>3.2 单数字重构效果</h2><h3 id="论文单数字重构效果"><a href="#论文单数字重构效果" class="headerlink" title="论文单数字重构效果:"></a>论文单数字重构效果:</h3><p><img src="https://img-blog.csdnimg.cn/20190615192737677.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="复现单数字重构效果"><a href="#复现单数字重构效果" class="headerlink" title="复现单数字重构效果:"></a>复现单数字重构效果:</h3><p><img src="https://img-blog.csdnimg.cn/20190615192749601.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="3-3-重叠数字重构效果"><a href="#3-3-重叠数字重构效果" class="headerlink" title="3.3 重叠数字重构效果"></a>3.3 重叠数字重构效果</h2><h3 id="论文重叠数字重构效果"><a href="#论文重叠数字重构效果" class="headerlink" title="论文重叠数字重构效果:"></a>论文重叠数字重构效果:</h3><p><img src="https://img-blog.csdnimg.cn/20190615192757872.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="复现重叠数字重构效果"><a href="#复现重叠数字重构效果" class="headerlink" title="复现重叠数字重构效果:"></a>复现重叠数字重构效果:</h3><p>第一行为实际图片和标签，第二行为预测的图片和标签，第三四行是把第二行两个图片分开的结果<br><img src="https://img-blog.csdnimg.cn/20190615192814738.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h1 id="4-github源码地址"><a href="#4-github源码地址" class="headerlink" title="4. github源码地址"></a>4. github源码地址</h1><blockquote><p><a href="https://github.com/wangjiosw/capsule-pytorch">https://github.com/wangjiosw/capsule-pytorch</a></p></blockquote><h1 id="5-参考文章"><a href="#5-参考文章" class="headerlink" title="5. 参考文章"></a>5. 参考文章</h1><p>[1] <a href="https://kexue.fm/archives/4819">揭开迷雾，来一顿美味的Capsule盛宴</a></p><p>[2] <a href="https://kexue.fm/archives/5112">再来一顿贺岁宴：从K-Means到Capsule</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;1-Capsule介绍&quot;&gt;&lt;a href=&quot;#1-Capsule介绍&quot; class=&quot;headerlink&quot; title=&quot;1. Capsule介绍&quot;&gt;&lt;/a&gt;1. Capsule介绍&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Sabour, Sara, Nicho</summary>
      
    
    
    
    <category term="深度学习" scheme="https://wangjiosw.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="cpasule" scheme="https://wangjiosw.github.io/tags/cpasule/"/>
    
    <category term="paper" scheme="https://wangjiosw.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>Mac OS VMware Fusion Centos6.5虚拟机网络设置</title>
    <link href="https://wangjiosw.github.io/2020/02/29/bigdata/environment/vmware/"/>
    <id>https://wangjiosw.github.io/2020/02/29/bigdata/environment/vmware/</id>
    <published>2020-02-29T15:25:00.000Z</published>
    <updated>2021-05-10T14:29:52.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-安装vmware虚拟机"><a href="#1-安装vmware虚拟机" class="headerlink" title="1. 安装vmware虚拟机"></a>1. 安装vmware虚拟机</h1><p>安装vmware虚拟机，并新建一个centos 64位的虚拟机</p><h1 id="2-设置虚拟机网络模式"><a href="#2-设置虚拟机网络模式" class="headerlink" title="2. 设置虚拟机网络模式"></a>2. 设置虚拟机网络模式</h1><p><img src="https://img-blog.csdnimg.cn/20200210222413551.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70"></p><h1 id="3-查看vmware的网关和掩码"><a href="#3-查看vmware的网关和掩码" class="headerlink" title="3. 查看vmware的网关和掩码"></a>3. 查看vmware的网关和掩码</h1><p>在Mac电脑的终端输入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> /Library/Preferences/VMware\ Fusion/vmnet8/nat.conf</span><br></pre></td></tr></table></figure><p>输出结果如下：<br><img src="https://img-blog.csdnimg.cn/20200210222624261.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70"><br>这里的ip和netmask即为vmware虚拟机的网关和掩码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># NAT gateway address</span></span><br><span class="line">ip = 172.16.143.2</span><br><span class="line">netmask = 255.255.255.0</span><br></pre></td></tr></table></figure><p>ip和netmask后面配置centos虚拟机的网络时分别对于网关和掩码。</p><h1 id="4-配置centos虚拟机的网络"><a href="#4-配置centos虚拟机的网络" class="headerlink" title="4. 配置centos虚拟机的网络"></a>4. 配置centos虚拟机的网络</h1><p>在centos虚拟机的终端输入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/sysconfig/network-scripts/ifcfg-eth0</span><br></pre></td></tr></table></figure><p>然后：</p><ul><li>删除UUID和MAC地址</li><li>ONBOOT=yes</li><li>BOOTPROTO=static</li><li>IPADDR=172.16.143.101</li><li>NETMASK=255.255.255.0</li><li>GATEWAY=172.16.143.2</li><li>DNS1=172.16.143.2</li></ul><p>保存并退出，然后在centos虚拟机的终端输入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service network restart</span><br></pre></td></tr></table></figure><p>ping 一下百度看是否能ping通：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ping www.baidu.com</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;1-安装vmware虚拟机&quot;&gt;&lt;a href=&quot;#1-安装vmware虚拟机&quot; class=&quot;headerlink&quot; title=&quot;1. 安装vmware虚拟机&quot;&gt;&lt;/a&gt;1. 安装vmware虚拟机&lt;/h1&gt;&lt;p&gt;安装vmware虚拟机，并新建一个centos </summary>
      
    
    
    
    <category term="虚拟机" scheme="https://wangjiosw.github.io/categories/%E8%99%9A%E6%8B%9F%E6%9C%BA/"/>
    
    
    <category term="vmware" scheme="https://wangjiosw.github.io/tags/vmware/"/>
    
    <category term="macos" scheme="https://wangjiosw.github.io/tags/macos/"/>
    
  </entry>
  
  <entry>
    <title>Torchtext使用教程</title>
    <link href="https://wangjiosw.github.io/2020/02/29/deep-learning/torchtext_use/"/>
    <id>https://wangjiosw.github.io/2020/02/29/deep-learning/torchtext_use/</id>
    <published>2020-02-29T13:25:00.000Z</published>
    <updated>2021-05-10T14:29:52.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Torchtext使用教程"><a href="#Torchtext使用教程" class="headerlink" title="Torchtext使用教程"></a>Torchtext使用教程</h1><h2 id="主要内容："><a href="#主要内容：" class="headerlink" title="主要内容："></a>主要内容：</h2><ul><li>如何使用torchtext建立语料库</li><li>如何使用torchtext将词转下标，下标转词，词转词向量</li><li>如何建立相应的迭代器</li></ul><h2 id="torchtext预处理流程："><a href="#torchtext预处理流程：" class="headerlink" title="torchtext预处理流程："></a>torchtext预处理流程：</h2><ol><li>定义Field：声明如何处理数据</li><li>定义Dataset：得到数据集，此时数据集里每一个样本是一个 经过 <strong>Field声明的预处理</strong> 预处理后的 wordlist</li><li>建立vocab：在这一步建立词汇表，词向量(word embeddings)</li><li>构造迭代器：构造迭代器，用来分批次训练模型</li></ol><h1 id="1-下载数据："><a href="#1-下载数据：" class="headerlink" title="1. 下载数据："></a>1. 下载数据：</h1><p><a href="https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-only/data">kaggle：Movie Review Sentiment Analysis (Kernels Only)</a><br>train.tsv contains the phrases and their associated sentiment labels. We have additionally provided a SentenceId so that you can track which phrases belong to a single sentence.</p><p>test.tsv contains just phrases. You must assign a sentiment label to each phrase.</p><p>The sentiment labels are:<br>0 - negative<br>1 - somewhat negative<br>2 - neutral<br>3 - somewhat positive<br>4 - positive</p><p>下载得到：train.tsv和test.tsv</p><h2 id="读取文件，查看文件"><a href="#读取文件，查看文件" class="headerlink" title="读取文件，查看文件"></a>读取文件，查看文件</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">data = pd.read_csv(&#x27;train.tsv&#x27;, sep=&#x27;\t&#x27;)</span><br><span class="line">test = pd.read_csv(&#x27;test.tsv&#x27;, sep=&#x27;\t&#x27;)</span><br></pre></td></tr></table></figure><h3 id="train-tsv"><a href="#train-tsv" class="headerlink" title="train.tsv"></a>train.tsv</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[:5]</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190619123238831.png"></p><h3 id="test-tsv"><a href="#test-tsv" class="headerlink" title="test.tsv"></a>test.tsv</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test[:5]</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190619123248113.png"></p><h1 id="2-划分验证集"><a href="#2-划分验证集" class="headerlink" title="2. 划分验证集"></a>2. 划分验证集</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"># create train and validation set </span><br><span class="line"></span><br><span class="line">train, val = train_test_split(data, test_size=0.2)</span><br><span class="line">train.to_csv(&quot;train.csv&quot;, index=False)</span><br><span class="line">val.to_csv(&quot;val.csv&quot;, index=False)</span><br></pre></td></tr></table></figure><h1 id="3-定义Field"><a href="#3-定义Field" class="headerlink" title="3. 定义Field"></a>3. 定义Field</h1><p>首先导入需要的包和定义pytorch张量使用的DEVICE</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import spacy</span><br><span class="line">import torch</span><br><span class="line">from torchtext import data, datasets</span><br><span class="line">from torchtext.vocab import Vectors</span><br><span class="line">from torch.nn import init</span><br><span class="line"></span><br><span class="line">DEVICE = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br></pre></td></tr></table></figure><p>Torchtext采用了一种声明式的方法来加载数据：你来告诉Torchtext你希望的数据是什么样子的，剩下的由torchtext来处理。<br>实现这种声明的是Field，Field确定了一种你想要怎么去处理数据。<br>data.Field(…)</p><p>Field的参数如下：</p><ul><li>sequential: Whether the datatype represents sequential data. If False, no tokenization is applied. Default: True.</li><li>use_vocab: Whether to use a Vocab object. If False, the data in this field should already be numerical. Default: True.</li><li>init_token: A token that will be prepended to every example using this field, or None for no initial token. Default: None.</li><li>eos_token: A token that will be appended to every example using this field, or None for no end-of-sentence token. Default: None.</li><li>fix_length: A fixed length that all examples using this field will be padded to, or None for flexible sequence lengths. Default: None.</li><li>dtype: The torch.dtype class that represents a batch of examples of this kind of data. Default: torch.long.</li><li>preprocessing: The Pipeline that will be applied to examples using this field after tokenizing but before numericalizing. Many Datasets replace this attribute with a custom preprocessor. Default: None.</li><li>postprocessing: A Pipeline that will be applied to examples using this field after numericalizing but before the numbers are turned into a Tensor. The pipeline function takes the batch as a list, and the field’s Vocab. Default: None.</li><li>lower: Whether to lowercase the text in this field. Default: False.</li><li>tokenize: The function used to tokenize strings using this field into sequential examples. If “spacy”, the SpaCy tokenizer is used. If a non-serializable function is passed as an argument, the field will not be able to be serialized. Default: string.split.</li><li>tokenizer_language: The language of the tokenizer to be constructed. Various languages currently supported only in SpaCy.</li><li>include_lengths: Whether to return a tuple of a padded minibatch and a list containing the lengths of each examples, or just a padded minibatch. Default: False.</li><li>batch_first: Whether to produce tensors with the batch dimension first. Default: False.</li><li>pad_token: The string token used as padding. Default: “<pad>“.</li><li>unk_token: The string token used to represent OOV words. Default: “<unk>“.</li><li>pad_first: Do the padding of the sequence at the beginning. Default: False.</li><li>truncate_first: Do the truncating of the sequence at the beginning. Default: False</li><li>stop_words: Tokens to discard during the preprocessing step. Default: None</li><li>is_target: Whether this field is a target variable. Affects iteration over batches. Default: False</li></ul><p><strong>例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">spacy_en = spacy.load(&#x27;en&#x27;)</span><br><span class="line"></span><br><span class="line">def tokenizer(text): # create a tokenizer function</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    定义分词操作</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return [tok.text for tok in spacy_en.tokenizer(text)]</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">field在默认的情况下都期望一个输入是一组单词的序列，并且将单词映射成整数。</span><br><span class="line">这个映射被称为vocab。如果一个field已经被数字化了并且不需要被序列化，</span><br><span class="line">可以将参数设置为use_vocab=False以及sequential=False。</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">LABEL = data.Field(sequential=False, use_vocab=False)</span><br><span class="line"></span><br><span class="line">TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="4-定义Dataset"><a href="#4-定义Dataset" class="headerlink" title="4. 定义Dataset"></a>4. 定义Dataset</h1><p>The fields知道当给定原始数据的时候要做什么。现在，我们需要告诉fields它需要处理什么样的数据。这个功能利用Datasets来实现。</p><p>Torchtext有大量内置的<a href="https://torchtext.readthedocs.io/en/latest/datasets.html">Datasets</a>去处理各种数据格式。</p><p><strong>TabularDataset官网介绍: Defines a Dataset of columns stored in CSV, TSV, or JSON format.</strong></p><p>对于csv/tsv类型的文件，TabularDataset很容易进行处理，故我们选它来生成Dataset</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;</span><br><span class="line">我们不需要 &#x27;PhraseId&#x27; 和 &#x27;SentenceId&#x27;这两列, 所以我们给他们的field传递 None</span><br><span class="line">如果你的数据有列名，如我们这里的&#x27;Phrase&#x27;,&#x27;Sentiment&#x27;,...</span><br><span class="line">设置skip_header=True,不然它会把列名也当一个数据处理</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">train,val = data.TabularDataset.splits(</span><br><span class="line">        path=&#x27;.&#x27;, train=&#x27;train.csv&#x27;,validation=&#x27;val.csv&#x27;, format=&#x27;csv&#x27;,skip_header=True,</span><br><span class="line">        fields=[(&#x27;PhraseId&#x27;,None),(&#x27;SentenceId&#x27;,None),(&#x27;Phrase&#x27;, TEXT), (&#x27;Sentiment&#x27;, LABEL)])</span><br><span class="line"></span><br><span class="line">test = data.TabularDataset(&#x27;test.tsv&#x27;, format=&#x27;tsv&#x27;,skip_header=True,</span><br><span class="line">        fields=[(&#x27;PhraseId&#x27;,None),(&#x27;SentenceId&#x27;,None),(&#x27;Phrase&#x27;, TEXT)])</span><br></pre></td></tr></table></figure><p><strong>注意：传入的(name, field)必须与列的顺序相同。</strong></p><p>查看生成的dataset：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(train[5])</span><br><span class="line">print(train[5].__dict__.keys())</span><br><span class="line">print(train[5].Phrase,train[0].Sentiment)</span><br></pre></td></tr></table></figure><p>输出：<br><img src="https://img-blog.csdnimg.cn/201906191233142.png"></p><h1 id="5-建立vocab"><a href="#5-建立vocab" class="headerlink" title="5. 建立vocab"></a>5. 建立vocab</h1><p>我们可以看到第6行的输入，它是一个Example对象。Example对象绑定了一行中的所有属性，可以看到，句子已经被分词了，但是没有转化为数字。</p><p>这是因为我们还没有建立vocab，我们将在下一步建立vocab。</p><p>Torchtext可以将词转化为数字，但是它需要被告知需要被处理的全部范围的词。我们可以用下面这行代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">TEXT.build_vocab(train, vectors=&#x27;glove.6B.100d&#x27;)#, max_size=30000)</span><br><span class="line"># 当 corpus 中有的 token 在 vectors 中不存在时 的初始化方式.</span><br><span class="line">TEXT.vocab.vectors.unk_init = init.xavier_uniform</span><br></pre></td></tr></table></figure><p>这行代码使得 Torchtext遍历<strong>训练集</strong>中的绑定TEXT field的数据，将单词注册到vocabulary，并自动构建embedding矩阵。</p><p><strong>‘glove.6B.100d’ 为torchtext支持的词向量名字，第一次使用是会自动下载并保存在当前目录的 .vector_cache里面。</strong></p><p><strong>torchtext支持的词向量</strong></p><ul><li>charngram.100d</li><li>fasttext.en.300d</li><li>fasttext.simple.300d</li><li>glove.42B.300d</li><li>glove.840B.300d</li><li>glove.twitter.27B.25d</li><li>glove.twitter.27B.50d</li><li>glove.twitter.27B.100d</li><li>glove.twitter.27B.200d</li><li>glove.6B.50d</li><li>glove.6B.100d</li><li>glove.6B.200d</li><li>glove.6B.300d</li></ul><p><strong>例：</strong></p><p>如果打算使用fasttext.en.300d词向量，只需把上面的代码里的vector=’…’里面的词向量名字换一下即可，具体如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TEXT.build_vocab(train, vectors=&#x27;fasttext.en.300d&#x27;)</span><br></pre></td></tr></table></figure><p>到这一步，我们已经可以把<strong>词转为数字，数字转为词，词转为词向量</strong>了</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">print(TEXT.vocab.itos[1510])</span><br><span class="line">print(TEXT.vocab.stoi[&#x27;bore&#x27;])</span><br><span class="line"># 词向量矩阵: TEXT.vocab.vectors</span><br><span class="line">print(TEXT.vocab.vectors.shape)</span><br><span class="line">word_vec = TEXT.vocab.vectors[TEXT.vocab.stoi[&#x27;bore&#x27;]]</span><br><span class="line">print(word_vec.shape)</span><br><span class="line">print(word_vec)</span><br></pre></td></tr></table></figure><p>输出：<br><img src="https://img-blog.csdnimg.cn/20190619123330249.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70"></p><h1 id="6-构造迭代器"><a href="#6-构造迭代器" class="headerlink" title="6. 构造迭代器"></a>6. 构造迭代器</h1><p>我们日常使用pytorch训练网络时，每次训练都是输入一个batch，那么，我们怎么把前面得到的dataset转为迭代器，然后遍历迭代器获取batch输入呢？下面将介绍torchtext时怎么实现这一功能的。</p><p>和Dataset一样，torchtext有大量内置的迭代器，我们这里选择的是BucketIterator，官网对它的介绍如下：</p><ul><li>Defines an iterator that batches examples of similar lengths together.</li><li>Minimizes amount of padding needed while producing freshly shuffled batches for each new epoch. </li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">train_iter = data.BucketIterator(train, batch_size=128, sort_key=lambda x: len(x.Phrase), </span><br><span class="line">                                 shuffle=True,device=DEVICE)</span><br><span class="line"></span><br><span class="line">val_iter = data.BucketIterator(val, batch_size=128, sort_key=lambda x: len(x.Phrase), </span><br><span class="line">                                 shuffle=True,device=DEVICE)</span><br><span class="line"></span><br><span class="line"># 在 test_iter , sort一定要设置成 False, 要不然会被 torchtext 搞乱样本顺序</span><br><span class="line">test_iter = data.Iterator(dataset=test, batch_size=128, train=False,</span><br><span class="line">                          sort=False, device=DEVICE)</span><br></pre></td></tr></table></figure><h2 id="迭代器使用"><a href="#迭代器使用" class="headerlink" title="迭代器使用"></a>迭代器使用</h2><h3 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">batch = next(iter(train_iter))</span><br><span class="line">data = batch.Phrase</span><br><span class="line">label = batch.Sentiment</span><br><span class="line">print(batch.Phrase.shape)</span><br><span class="line">print(batch.Phrase)</span><br></pre></td></tr></table></figure><p>输出结果：<br><img src="https://img-blog.csdnimg.cn/20190619123347960.png"><br>可以发现，它输出的是word index，后面的128是batch size</p><h3 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for batch in train_iter:</span><br><span class="line">    data = batch.Phrase</span><br><span class="line">    label = batch.Sentiment</span><br></pre></td></tr></table></figure><h1 id="7-完整代码"><a href="#7-完整代码" class="headerlink" title="7. 完整代码"></a>7. 完整代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> data, datasets</span><br><span class="line"><span class="keyword">from</span> torchtext.vocab <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">DEVICE = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(<span class="string">&#x27;train.tsv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">test = pd.read_csv(<span class="string">&#x27;test.tsv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create train and validation set </span></span><br><span class="line">train, val = train_test_split(data, test_size=<span class="number">0.2</span>)</span><br><span class="line">train.to_csv(<span class="string">&quot;train.csv&quot;</span>, index=<span class="literal">False</span>)</span><br><span class="line">val.to_csv(<span class="string">&quot;val.csv&quot;</span>, index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">spacy_en = spacy.load(<span class="string">&#x27;en&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenizer</span>(<span class="params">text</span>): <span class="comment"># create a tokenizer function</span></span><br><span class="line">    <span class="keyword">return</span> [tok.text <span class="keyword">for</span> tok <span class="keyword">in</span> spacy_en.tokenizer(text)]</span><br><span class="line"><span class="comment"># Field</span></span><br><span class="line">TEXT = data.Field(sequential=<span class="literal">True</span>, tokenize=tokenizer, lower=<span class="literal">True</span>)</span><br><span class="line">LABEL = data.Field(sequential=<span class="literal">False</span>, use_vocab=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Dataset</span></span><br><span class="line">train,val = data.TabularDataset.splits(</span><br><span class="line">        path=<span class="string">&#x27;.&#x27;</span>, train=<span class="string">&#x27;train.csv&#x27;</span>,validation=<span class="string">&#x27;val.csv&#x27;</span>, <span class="built_in">format</span>=<span class="string">&#x27;csv&#x27;</span>,skip_header=<span class="literal">True</span>,</span><br><span class="line">        fields=[(<span class="string">&#x27;PhraseId&#x27;</span>,<span class="literal">None</span>),(<span class="string">&#x27;SentenceId&#x27;</span>,<span class="literal">None</span>),(<span class="string">&#x27;Phrase&#x27;</span>, TEXT), (<span class="string">&#x27;Sentiment&#x27;</span>, LABEL)])</span><br><span class="line"></span><br><span class="line">test = data.TabularDataset(<span class="string">&#x27;test.tsv&#x27;</span>, <span class="built_in">format</span>=<span class="string">&#x27;tsv&#x27;</span>,skip_header=<span class="literal">True</span>,</span><br><span class="line">        fields=[(<span class="string">&#x27;PhraseId&#x27;</span>,<span class="literal">None</span>),(<span class="string">&#x27;SentenceId&#x27;</span>,<span class="literal">None</span>),(<span class="string">&#x27;Phrase&#x27;</span>, TEXT)])</span><br><span class="line"><span class="comment"># build vocab</span></span><br><span class="line">TEXT.build_vocab(train, vectors=<span class="string">&#x27;glove.6B.100d&#x27;</span>)<span class="comment">#, max_size=30000)</span></span><br><span class="line">TEXT.vocab.vectors.unk_init = init.xavier_uniform</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterator</span></span><br><span class="line">train_iter = data.BucketIterator(train, batch_size=<span class="number">128</span>, sort_key=<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x.Phrase), </span><br><span class="line">                                 shuffle=<span class="literal">True</span>,device=DEVICE)</span><br><span class="line"></span><br><span class="line">val_iter = data.BucketIterator(val, batch_size=<span class="number">128</span>, sort_key=<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x.Phrase), </span><br><span class="line">                                 shuffle=<span class="literal">True</span>,device=DEVICE)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 test_iter , sort一定要设置成 False, 要不然会被 torchtext 搞乱样本顺序</span></span><br><span class="line">test_iter = data.Iterator(dataset=test, batch_size=<span class="number">128</span>, train=<span class="literal">False</span>,</span><br><span class="line">                          sort=<span class="literal">False</span>, device=DEVICE)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">由于目的是学习torchtext的使用，所以只定义了一个简单模型</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">len_vocab = <span class="built_in">len</span>(TEXT.vocab)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Enet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Enet, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(len_vocab,<span class="number">100</span>)</span><br><span class="line">        self.lstm = nn.LSTM(<span class="number">100</span>,<span class="number">128</span>,<span class="number">3</span>,batch_first=<span class="literal">True</span>)<span class="comment">#,bidirectional=True)</span></span><br><span class="line">        self.linear = nn.Linear(<span class="number">128</span>,<span class="number">5</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        batch_size,seq_num = x.shape</span><br><span class="line">        vec = self.embedding(x)</span><br><span class="line">        out, (hn, cn) = self.lstm(vec)</span><br><span class="line">        out = self.linear(out[:,-<span class="number">1</span>,:])</span><br><span class="line">        out = F.softmax(out,-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Enet()</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">将前面生成的词向量矩阵拷贝到模型的embedding层</span></span><br><span class="line"><span class="string">这样就自动的可以将输入的word index转为词向量</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">model.embedding.weight.data.copy_(TEXT.vocab.vectors)   </span><br><span class="line">model.to(DEVICE)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">optimizer = optim.Adam(model.parameters())<span class="comment">#,lr=0.000001)</span></span><br><span class="line"></span><br><span class="line">n_epoch = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">best_val_acc = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epoch):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch_idx, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">        data = batch.Phrase</span><br><span class="line">        target = batch.Sentiment</span><br><span class="line">        target = torch.sparse.torch.eye(<span class="number">5</span>).index_select(dim=<span class="number">0</span>, index=target.cpu().data)</span><br><span class="line">        target = target.to(DEVICE)</span><br><span class="line">        data = data.permute(<span class="number">1</span>,<span class="number">0</span>)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        out = model(data)</span><br><span class="line">        loss = -target*torch.log(out)-(<span class="number">1</span>-target)*torch.log(<span class="number">1</span>-out)</span><br><span class="line">        loss = loss.<span class="built_in">sum</span>(-<span class="number">1</span>).mean()</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (batch_idx+<span class="number">1</span>) %<span class="number">200</span> == <span class="number">0</span>:</span><br><span class="line">            _,y_pre = torch.<span class="built_in">max</span>(out,-<span class="number">1</span>)</span><br><span class="line">            acc = torch.mean((torch.tensor(y_pre == batch.Sentiment,dtype=torch.<span class="built_in">float</span>)))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;epoch: %d \t batch_idx : %d \t loss: %.4f \t train acc: %.4f&#x27;</span></span><br><span class="line">                  %(epoch,batch_idx,loss,acc))</span><br><span class="line">    </span><br><span class="line">    val_accs = []</span><br><span class="line">    <span class="keyword">for</span> batch_idx, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(val_iter):</span><br><span class="line">        data = batch.Phrase</span><br><span class="line">        target = batch.Sentiment</span><br><span class="line">        target = torch.sparse.torch.eye(<span class="number">5</span>).index_select(dim=<span class="number">0</span>, index=target.cpu().data)</span><br><span class="line">        target = target.to(DEVICE)</span><br><span class="line">        data = data.permute(<span class="number">1</span>,<span class="number">0</span>)</span><br><span class="line">        out = model(data)</span><br><span class="line">        </span><br><span class="line">        _,y_pre = torch.<span class="built_in">max</span>(out,-<span class="number">1</span>)</span><br><span class="line">        acc = torch.mean((torch.tensor(y_pre == batch.Sentiment,dtype=torch.<span class="built_in">float</span>)))</span><br><span class="line">        val_accs.append(acc)</span><br><span class="line">    </span><br><span class="line">    acc = np.array(val_accs).mean()</span><br><span class="line">    <span class="keyword">if</span> acc &gt; best_val_acc:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;val acc : %.4f &gt; %.4f saving model&#x27;</span>%(acc,best_val_acc))</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">&#x27;params.pkl&#x27;</span>)</span><br><span class="line">        best_val_acc = acc</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;val acc: %.4f&#x27;</span>%(acc))</span><br></pre></td></tr></table></figure><h1 id="8-参考"><a href="#8-参考" class="headerlink" title="8. 参考"></a>8. 参考</h1><ul><li><a href="https://zhuanlan.zhihu.com/p/65833208">pytorch学习笔记—Torchtext</a></li><li><a href="https://zhuanlan.zhihu.com/p/34722385">使用 torchtext 做 Toxic Comment Classification 比赛的数据预处理</a></li><li><a href="https://towardsdatascience.com/how-to-use-torchtext-for-neural-machine-translation-plus-hack-to-make-it-5x-faster-77f3884d95">How to use TorchText for neural machine translation, plus hack to make it 5x faster</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Torchtext使用教程&quot;&gt;&lt;a href=&quot;#Torchtext使用教程&quot; class=&quot;headerlink&quot; title=&quot;Torchtext使用教程&quot;&gt;&lt;/a&gt;Torchtext使用教程&lt;/h1&gt;&lt;h2 id=&quot;主要内容：&quot;&gt;&lt;a href=&quot;#主要内容</summary>
      
    
    
    
    <category term="深度学习" scheme="https://wangjiosw.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="nlp" scheme="https://wangjiosw.github.io/tags/nlp/"/>
    
    <category term="torchtext" scheme="https://wangjiosw.github.io/tags/torchtext/"/>
    
    <category term="python" scheme="https://wangjiosw.github.io/tags/python/"/>
    
  </entry>
  
</feed>
